# ==========================================================
# DATA MINING PROJECT - GOLD PRICE DATA
# Dataset: goldstock v2.csv
# B1 ‚Üí B5 (EDA-focused, model minh h·ªça, c√≥ GUI)
# ==========================================================

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as pltt

from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.metrics import mean_absolute_error, mean_squared_error, silhouette_score, davies_bouldin_score, calinski_harabasz_score
from scipy import stats


plt.style.use('ggplot')

# ==========================================================
# LOAD DATA - B0: DATASET OVERVIEW
# ==========================================================
original_df = pd.read_csv("goldstock v2.csv")

# X√≥a c·ªôt index kh√¥ng c·∫ßn thi·∫øt
if "Unnamed: 0" in original_df.columns:
    original_df.drop(columns=["Unnamed: 0"], inplace=True)

# Chuy·ªÉn Date sang datetime
original_df["Date"] = pd.to_datetime(original_df["Date"])

# S·∫Øp x·∫øp theo th·ªùi gian
original_df.sort_values(by="Date", inplace=True)
original_df.reset_index(drop=True, inplace=True)
# ===== Convert c√°c c·ªôt s·ªë v·ªÅ numeric (ph√≤ng tr∆∞·ªùng h·ª£p c√≥ '$' v√† ',' ) =====
def to_numeric_clean(s: pd.Series):
    if s.dtype == "O":  # object/string
        s = (s.astype(str)
               .str.replace("$", "", regex=False)
               .str.replace(",", "", regex=False)
               .str.strip())
    return pd.to_numeric(s, errors="coerce")

for col in ["Open", "High", "Low", "Close/Last", "Volume"]:
    if col in original_df.columns:
        original_df[col] = to_numeric_clean(original_df[col])

# ===== CLEANING B·ªî SUNG SAU CONVERT NUMERIC =====

# Drop Date l·ªói
original_df = original_df.dropna(subset=["Date"]).copy()

# Drop NaN ph√°t sinh sau khi convert numeric
num_cols = ["Open", "High", "Low", "Close/Last", "Volume"]
num_cols = [c for c in num_cols if c in original_df.columns]
original_df = original_df.dropna(subset=num_cols).copy()

# Volume ph·∫£i kh√¥ng √¢m
if "Volume" in original_df.columns:
    original_df = original_df[original_df["Volume"] >= 0].copy()


# ==========================================================
# B2 ‚Äì DATA CLEANING (TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU)
# ==========================================================
# Clone m·ªôt b·∫£n sao ƒë·ªÉ gi·ªØ nguy√™n d·ªØ li·ªáu g·ªëc
df = original_df.copy()

# Ki·ªÉm tra d·ªØ li·ªáu thi·∫øu
missing_data = df.isnull().sum() * 100 / df.shape[0]

# X√≥a duplicate
# Remove duplicate theo Date (chu·∫©n time-series)
df = df.sort_values("Date").drop_duplicates(subset=["Date"], keep="last").reset_index(drop=True)


# Ki·ªÉm tra logic gi√° (High >= Open, Close, Low; Low <= Open, Close)
df = df[
    (df["High"] >= df["Open"]) &
    (df["High"] >= df["Close/Last"]) &
    (df["High"] >= df["Low"]) &
    (df["Low"] <= df["Open"]) &
    (df["Low"] <= df["Close/Last"])
]
# ===== VALIDATION B·ªî SUNG =====

# Gi√° ph·∫£i d∆∞∆°ng
for c in ["Open", "High", "Low", "Close/Last"]:
    df = df[df[c] > 0]

# Close v√† Open ph·∫£i n·∫±m trong [Low, High]
df = df[
    (df["Close/Last"] >= df["Low"]) &
    (df["Close/Last"] <= df["High"]) &
    (df["Open"] >= df["Low"]) &
    (df["Open"] <= df["High"])
]

df.reset_index(drop=True, inplace=True)

df.reset_index(drop=True, inplace=True)

# ==========================================================
# B1 ‚Äì M√î T·∫¢ D·ªÆ LI·ªÜU (DATA OVERVIEW)
# ==========================================================
quantitative_cols = df.select_dtypes(include=["int64", "float64"]).columns.tolist()
qualitative_cols = df.select_dtypes(exclude=["int64", "float64"]).columns.tolist()

# ==========================================================
# B5+ (SUPERVISED) ‚Äì LINEAR REGRESSION FORECAST (N√ÇNG C·∫§P)
# ==========================================================
def make_lr_dataset(df_in: pd.DataFrame, horizon: int = 1):
    """
    horizon=1: d·ª± ƒëo√°n Close(t+1)
    horizon=7: d·ª± ƒëo√°n Close(t+7)
    
    ‚úÖ C·∫¢I THI·ªÜN: Th√™m nhi·ªÅu feature ch·∫•t l∆∞·ª£ng cao (momentum, ROC, RSI, Bollinger Bands)
    """
    d = df_in.copy()

    # Target: future return (thay v√¨ absolute price - d·ªÖ scale h∆°n)
    d["y"] = d["Close/Last"].shift(-horizon) / d["Close/Last"] - 1

    # ===== LAG FEATURES (gi·ªØ nguy√™n) =====
    for lag in [1, 2, 3, 5, 7, 14, 30]:
        d[f"close_lag_{lag}"] = d["Close/Last"].shift(lag)

    # ===== ROLLING FEATURES (N√ÇNG C·∫§P) =====
    d["ma_7"] = d["Close/Last"].rolling(7).mean()
    d["ma_14"] = d["Close/Last"].rolling(14).mean()
    d["ma_20"] = d["Close/Last"].rolling(20).mean()
    d["ma_30"] = d["Close/Last"].rolling(30).mean()
    d["ma_60"] = d["Close/Last"].rolling(60).mean()
    d["ma_200"] = d["Close/Last"].rolling(200).mean()
    
    d["std_7"] = d["Close/Last"].rolling(7).std()
    d["std_14"] = d["Close/Last"].rolling(14).std()
    d["std_20"] = d["Close/Last"].rolling(20).std()
    
    # ===== MOMENTUM & TREND FEATURES (T·ªêI QUAN TR·ªåNG - C·∫¢I THI·ªÜN) =====
    d["momentum_7"] = d["Close/Last"] - d["Close/Last"].shift(7)
    d["momentum_14"] = d["Close/Last"] - d["Close/Last"].shift(14)
    d["momentum_30"] = d["Close/Last"] - d["Close/Last"].shift(30)
    
    # Rate of change (t·ªëc ƒë·ªô thay ƒë·ªïi %)
    d["roc_7"] = (d["Close/Last"] - d["Close/Last"].shift(7)) / d["Close/Last"].shift(7)
    d["roc_14"] = (d["Close/Last"] - d["Close/Last"].shift(14)) / d["Close/Last"].shift(14)
    d["roc_30"] = (d["Close/Last"] - d["Close/Last"].shift(30)) / d["Close/Last"].shift(30)
    
    # RSI-like oscillator (0-100)
    delta = d["Close/Last"].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    d["rsi_14"] = 100 - (100 / (1 + rs))
    
    # Bollinger Bands (ƒë·ªô l·ªách so v·ªõi MA)
    d["bb_upper"] = d["ma_20"] + 2 * d["std_20"]
    d["bb_lower"] = d["ma_20"] - 2 * d["std_20"]
    d["bb_position"] = (d["Close/Last"] - d["bb_lower"]) / (d["bb_upper"] - d["bb_lower"])
    
    # ===== VOLUME FEATURES (C·∫¨P NH·∫¨T) =====
    if "Volume" in d.columns:
        d["vol_lag_1"] = d["Volume"].shift(1)
        d["vol_ma_7"] = d["Volume"].rolling(7).mean()
        d["vol_ma_30"] = d["Volume"].rolling(30).mean()
        d["vol_std"] = d["Volume"].rolling(7).std()
        
        # Volume-Price Trend
        d["price_vol_trend"] = (d["Close/Last"] - d["Close/Last"].shift(1)) / d["Close/Last"].shift(1) * d["Volume"]

    # ===== LO·∫†I B·ªé NaN & CH·ªåN FEATURES =====
    d = d.dropna().reset_index(drop=True)

    feature_cols = [c for c in d.columns if c.startswith(("close_lag_", "ma_", "std_", "vol_", "momentum_", "roc_", "rsi_", "bb_", "price_"))]
    return d, feature_cols


def time_split(d: pd.DataFrame, test_ratio: float = 0.2):
    n = len(d)
    test_n = max(1, int(n * test_ratio))
    train = d.iloc[:-test_n].copy()
    test = d.iloc[-test_n:].copy()
    return train, test


# ...existing code...

# ==========================================================
# STREAMLIT GUI CONFIGURATION
# ==========================================================
st.set_page_config(page_title="Gold Price Data Mining", layout="wide")
st.title("üìä Gold Price Data Mining Project")
st.markdown("---")

# Create tabs for each phase
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "B1 - Data Overview",
    "B2 - Data Cleaning",
    "B3 - Exploratory Analysis",
    "B4 - Correlation & Dimensionality",
    "B5 - Model & Visualization"
])

# ==========================================================
# TAB 1: B1 ‚Äì DATA OVERVIEW (M√î T·∫¢ D·ªÆ LI·ªÜU)
# ==========================================================
with tab1:
    st.header("B1 ‚Äì M√¥ t·∫£ d·ªØ li·ªáu (Dataset Overview)")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total Rows", df.shape[0])
    with col2:
        st.metric("Total Columns", df.shape[1])
    with col3:
        st.metric("Date Range", f"{df['Date'].min().date()} to {df['Date'].max().date()}")
    
    st.write("### üìã Danh s√°ch c√°c c·ªôt (Dataset Columns)")
    st.write(df.columns.tolist())
    
    st.write("### üìä Ph√¢n lo·∫°i d·ªØ li·ªáu (Data Types Classification)")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**ƒê·ªãnh l∆∞·ª£ng (Quantitative):**")
        st.write(f"- S·ªë c·ªôt: **{len(quantitative_cols)}**")
        for col in quantitative_cols:
            st.write(f"  - `{col}` ({df[col].dtype})")
    
    with col2:
        st.write("**ƒê·ªãnh t√≠nh (Qualitative):**")
        st.write(f"- S·ªë c·ªôt: **{len(qualitative_cols)}**")
        for col in qualitative_cols:
            st.write(f"  - `{col}` ({df[col].dtype})")
    
    st.write("### üìà Th·ªëng k√™ m√¥ t·∫£ chi ti·∫øt (Descriptive Statistics)")
    st.dataframe(df[quantitative_cols].describe(), use_container_width=True)
    
    st.write("### üîç Th√¥ng tin chi ti·∫øt c√°c c·ªôt (Detailed Column Info)")
    info_data = {
        "Column": df.columns,
        "Data Type": df.dtypes.astype(str),
        "Non-Null Count": df.count(),
        "Null Count": df.isnull().sum(),
        "Min": [df[col].min() if col in quantitative_cols else "N/A" for col in df.columns],
        "Max": [df[col].max() if col in quantitative_cols else "N/A" for col in df.columns],
    }
    st.dataframe(pd.DataFrame(info_data), use_container_width=True)
    
    st.write("### üìù D·ªØ li·ªáu m·∫´u (Sample Data)")
    st.dataframe(df.head(10), use_container_width=True)

# ==========================================================
# TAB 2: B2 ‚Äì DATA CLEANING (TI·ªÄN X·ª¨ L√ù)
# ==========================================================
with tab2:
    st.header("B2 ‚Äì Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu (Data Cleaning)")
    
    st.write("### üîç 1. Ki·ªÉm tra d·ªØ li·ªáu thi·∫øu (Missing Data)")
    missing_count = original_df.isnull().sum()
    missing_percent = (missing_count / len(original_df)) * 100
    missing_df = pd.DataFrame({
        "Column": missing_count.index,
        "Missing Count": missing_count.values,
        "Missing %": missing_percent.values
    })
    st.dataframe(missing_df[missing_df["Missing Count"] > 0] if missing_df["Missing Count"].sum() > 0 
                 else pd.DataFrame({"Status": ["‚úÖ No missing data found"]}), use_container_width=True)
    
    st.write("### üîÑ 2. Ki·ªÉm tra d·ªØ li·ªáu tr√πng l·∫∑p (Duplicate Data)")
    col1, col2 = st.columns(2)
    with col1:
        st.write(f"‚ùå **Tr∆∞·ªõc x·ª≠ l√Ω:** {original_df.duplicated().sum()} d√≤ng tr√πng l·∫∑p")
    with col2:
        st.write(f"‚úÖ **Sau x·ª≠ l√Ω:** {df.duplicated().sum()} d√≤ng tr√πng l·∫∑p")
    
    st.write("### üö® 3. Ph√°t hi·ªán Noise & Outliers")
    
    st.write("### üìå Outlier theo Return (khuy·∫øn ngh·ªã cho d·ªØ li·ªáu t√†i ch√≠nh)")

    # 1) Check column
    if "Close/Last" not in df.columns:
        st.error("Kh√¥ng t√¨m th·∫•y c·ªôt 'Close/Last' trong df.")
    else:
        # 2) √âp ki·ªÉu numeric an to√†n
        close = pd.to_numeric(df["Close/Last"], errors="coerce")

        # 3) T√≠nh return + l√†m s·∫°ch NaN/Inf
        ret = close.pct_change()
        ret = ret.replace([np.inf, -np.inf], np.nan).dropna()

        # 4) N·∫øu d·ªØ li·ªáu qu√° √≠t th√¨ c·∫£nh b√°o
        if len(ret) < 30:
            st.warning(f"Return h·ª£p l·ªá qu√° √≠t ({len(ret)} ƒëi·ªÉm) n√™n th·ªëng k√™ outlier kh√¥ng ƒë√°ng tin.")
        else:
            q1, q3 = ret.quantile(0.25), ret.quantile(0.75)
            iqr = q3 - q1
            lb, ub = q1 - 1.5 * iqr, q3 + 1.5 * iqr

            out_ret = ((ret < lb) | (ret > ub)).sum()
            st.write(f"- S·ªë outlier theo return (IQR): **{out_ret}**")
            st.write(f"- Ng∆∞·ª°ng IQR: **[{lb:.4f}, {ub:.4f}]**")

            st.info("Outlier gi√° KH√îNG n√™n x√≥a v√¨ ph·∫£n √°nh bi·∫øn ƒë·ªông th·ªã tr∆∞·ªùng. N·∫øu c·∫ßn ·ªïn ƒë·ªãnh m√¥ h√¨nh, c√¢n nh·∫Øc x·ª≠ l√Ω outlier tr√™n return/feature.")
        
        # Ki·ªÉm tra outliers b·∫±ng IQR
        st.write("**Ph∆∞∆°ng ph√°p IQR (Interquartile Range):**")
        outlier_summary = []
        
        for col in quantitative_cols:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outlier_count = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
            outlier_summary.append({
                "Column": col,
                "Q1": round(Q1, 2),
                "Q3": round(Q3, 2),
                "IQR": round(IQR, 2),
                "Lower Bound": round(lower_bound, 2),
                "Upper Bound": round(upper_bound, 2),
                "Outlier Count": outlier_count
            })
        
        outlier_df = pd.DataFrame(outlier_summary)
        st.dataframe(outlier_df, use_container_width=True)
        
        # Visualize outliers
        fig, axes = plt.subplots(len(quantitative_cols), 1, figsize=(10, 3*len(quantitative_cols)))
        if len(quantitative_cols) == 1:
            axes = [axes]
        
        for idx, col in enumerate(quantitative_cols):
            sns.boxplot(data=df, y=col, ax=axes[idx], color='steelblue')
            axes[idx].set_title(f"Outlier Detection: {col}")
        
        plt.tight_layout()
        st.pyplot(fig)
        
        st.write("### ‚úÖ 4. C√°c quy t·∫Øc x√°c th·ª±c d·ªØ li·ªáu (Data Validation Rules)")
        st.write("""
        ‚úîÔ∏è **High >= Open, Close/Last, Low** - Gi√° cao nh·∫•t >= t·∫•t c·∫£ c√°c m·ª©c gi√° kh√°c
        ‚úîÔ∏è **Low <= Open, Close/Last** - Gi√° th·∫•p nh·∫•t <= t·∫•t c·∫£ c√°c m·ª©c gi√° kh√°c
        ‚úîÔ∏è **Volume >= 0** - Kh·ªëi l∆∞·ª£ng kh√¥ng √¢m
        ‚úîÔ∏è **Date sorted in ascending order** - D·ªØ li·ªáu s·∫Øp x·∫øp theo th·ªùi gian
        ‚úîÔ∏è **Duplicates removed** - Lo·∫°i b·ªè d√≤ng tr√πng l·∫∑p
        """)
        
        st.write("### üìä 5. So s√°nh d·ªØ li·ªáu tr∆∞·ªõc & sau l√†m s·∫°ch")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Rows (Before)", original_df.shape[0])
        with col2:
            st.metric("Rows (After)", df.shape[0])
        with col3:
            st.metric("Rows Removed", original_df.shape[0] - df.shape[0])
        
        st.write("### üìù D·ªØ li·ªáu m·∫´u sau l√†m s·∫°ch (Sample Cleaned Data)")
        st.dataframe(df.head(10), use_container_width=True)

# ==========================================================
# TAB 3: B3 ‚Äì EXPLORATORY DATA ANALYSIS (KHAI PH√Å D·ªÆ LI·ªÜU)
# ==========================================================
with tab3:
    st.header("B3 ‚Äì Khai ph√° d·ªØ li·ªáu (Exploratory Data Analysis)")
    
    # ===== UNIVARIATE ANALYSIS =====
    st.subheader("üìä Ph√¢n t√≠ch ƒë∆°n bi·∫øn (Univariate Analysis)")
    
    col_univariate = st.selectbox(
        "Ch·ªçn c·ªôt ƒë·ªãnh l∆∞·ª£ng ƒë·ªÉ ph√¢n t√≠ch",
        quantitative_cols,
        key="univariate"
    )
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Histogram with KDE
        fig, ax = plt.subplots(figsize=(10, 5))
        sns.histplot(df[col_univariate], kde=True, ax=ax, color='steelblue', bins=30)
        ax.set_title(f"Distribution of {col_univariate}")
        ax.set_xlabel(col_univariate)
        ax.set_ylabel("Frequency")
        st.pyplot(fig)
    
    with col2:
        # Box plot
        fig, ax = plt.subplots(figsize=(10, 5))
        sns.boxplot(data=df, y=col_univariate, ax=ax, color='lightcoral')
        ax.set_title(f"Box Plot of {col_univariate}")
        st.pyplot(fig)
    
    # Statistical summary
    st.write(f"### üìà Th·ªëng k√™ m√¥ t·∫£: {col_univariate}")
    stats_data = {
        "Metric": ["Count", "Mean", "Std Dev", "Min", "25%", "Median", "75%", "Max", "Range", "Skewness"],
        "Value": [
            df[col_univariate].count(),
            round(df[col_univariate].mean(), 2),
            round(df[col_univariate].std(), 2),
            round(df[col_univariate].min(), 2),
            round(df[col_univariate].quantile(0.25), 2),
            round(df[col_univariate].median(), 2),
            round(df[col_univariate].quantile(0.75), 2),
            round(df[col_univariate].max(), 2),
            round(df[col_univariate].max() - df[col_univariate].min(), 2),
            round(df[col_univariate].skew(), 2),
        ]
    }
    st.dataframe(pd.DataFrame(stats_data), use_container_width=True)
    
    # ===== BIVARIATE ANALYSIS =====
    st.subheader("üìà Ph√¢n t√≠ch ƒëa bi·∫øn (Bivariate Analysis)")
    
    col1, col2 = st.columns(2)
    with col1:
        x_col = st.selectbox("Ch·ªçn tr·ª•c X", quantitative_cols, index=0, key="x_axis")
    with col2:
        y_col = st.selectbox("Ch·ªçn tr·ª•c Y", quantitative_cols, index=1 if len(quantitative_cols) > 1 else 0, key="y_axis")
    
    fig, ax = plt.subplots(figsize=(10, 6))
    sns.scatterplot(x=df[x_col], y=df[y_col], ax=ax, alpha=0.6, color='steelblue', s=50)
    # Th√™m trendline
    z = np.polyfit(df[x_col], df[y_col], 1)
    p = np.poly1d(z)
    ax.plot(df[x_col].sort_values(), p(df[x_col].sort_values()), "r--", linewidth=2, label='Trend')
    ax.set_title(f"Scatter Plot: {x_col} vs {y_col}")
    ax.legend()
    st.pyplot(fig)
    
    # Correlation coefficient
    corr_coef = df[x_col].corr(df[y_col])
    st.write(f"**H·ªá s·ªë t∆∞∆°ng quan Pearson:** {corr_coef:.4f}")
    
    # ===== TIME SERIES ANALYSIS =====
    st.subheader("‚è∞ Ph√¢n t√≠ch chu·ªói th·ªùi gian (Time Series Analysis)")
    
    fig, ax = plt.subplots(figsize=(12, 5))
    ax.plot(df["Date"], df["Close/Last"], linewidth=2, color='steelblue', label='Close Price')
    ax.fill_between(df["Date"], df["Low"], df["High"], alpha=0.2, color='lightblue', label='High-Low Range')
    ax.set_xlabel("Date")
    ax.set_ylabel("Price (USD/oz)")
    ax.set_title("Xu h∆∞·ªõng gi√° v√†ng (Gold Price Trend)")
    ax.legend()
    plt.xticks(rotation=45)
    st.pyplot(fig)
    
    # Th·ªëng k√™ chu·ªói th·ªùi gian
    st.write("### üìä Th·ªëng k√™ chu·ªói th·ªùi gian:")
    time_stats = {
        "Metric": ["Avg Close Price", "Max Close Price", "Min Close Price", "Price Range", "Volatility (Std)"],
        "Value": [
            f"${df['Close/Last'].mean():.2f}",
            f"${df['Close/Last'].max():.2f}",
            f"${df['Close/Last'].min():.2f}",
            f"${df['Close/Last'].max() - df['Close/Last'].min():.2f}",
            f"${df['Close/Last'].std():.2f}"
        ]
    }
    st.dataframe(pd.DataFrame(time_stats), use_container_width=True)
    
    # ===== VOLUME ANALYSIS =====
    st.subheader("üìä Ph√¢n t√≠ch kh·ªëi l∆∞·ª£ng giao d·ªãch (Volume Analysis)")
    
    fig, ax = plt.subplots(figsize=(12, 5))
    ax.bar(df["Date"], df["Volume"], color='steelblue', alpha=0.7, width=0.8)
    ax.set_xlabel("Date")
    ax.set_ylabel("Volume")
    ax.set_title("Kh·ªëi l∆∞·ª£ng giao d·ªãch (Trading Volume Over Time)")
    plt.xticks(rotation=45)
    st.pyplot(fig)
    
    # Volume statistics
    st.write("### üìä Th·ªëng k√™ kh·ªëi l∆∞·ª£ng:")
    volume_stats = {
        "Metric": ["Avg Volume", "Max Volume", "Min Volume", "Total Volume"],
        "Value": [
            f"{df['Volume'].mean():.0f}",
            f"{df['Volume'].max():.0f}",
            f"{df['Volume'].min():.0f}",
            f"{df['Volume'].sum():.0f}"
        ]
    }
    st.dataframe(pd.DataFrame(volume_stats), use_container_width=True)

# ==========================================================
# TAB 4: B4 ‚Äì CORRELATION & DIMENSIONALITY REDUCTION
# ==========================================================
with tab4:
    st.header("B4 ‚Äì Ma tr·∫≠n t∆∞∆°ng quan & Gi·∫£m chi·ªÅu (Correlation & PCA)")
    
    # ===== CORRELATION MATRIX =====
    st.subheader("üîó Ma tr·∫≠n t∆∞∆°ng quan (Correlation Matrix)")
    
    corr_matrix = df[quantitative_cols].corr()
    
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", center=0, 
                square=True, ax=ax, fmt=".2f", cbar_kws={'label': 'Correlation'})
    ax.set_title("Correlation Matrix of Quantitative Variables")
    st.pyplot(fig)
    
    st.write("### üß† Ph√¢n t√≠ch & L·∫≠p lu·∫≠n Gi·ªØ/B·ªè c·ªôt (Analysis & Column Selection Reasoning)")
    
    # T√≠nh t∆∞∆°ng quan cao
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > 0.95:
                high_corr_pairs.append({
                    "Column 1": corr_matrix.columns[i],
                    "Column 2": corr_matrix.columns[j],
                    "Correlation": round(corr_matrix.iloc[i, j], 4)
                })
    
    if high_corr_pairs:
        st.write("**C√°c c·ªôt c√≥ t∆∞∆°ng quan r·∫•t cao (> 0.95):**")
        st.dataframe(pd.DataFrame(high_corr_pairs), use_container_width=True)
    
    st.write("""
    **K·∫øt lu·∫≠n & Quy·∫øt ƒë·ªãnh:**
    
    1. **Open, High, Low, Close/Last** c√≥ t∆∞∆°ng quan r·∫•t cao (> 0.95)
       - ‚ùå Gi·ªØ t·∫•t c·∫£ 4 c·ªôt l√† d∆∞ th·ª´a
       - ‚úÖ **Gi·ªØ:** `Close/Last` (gi√° ƒë√≥ng c·ª≠a - ch·ªâ b√°o ch√≠nh)
       - ‚ùå **B·ªè:** `Open`, `High`, `Low` (c√≥ th·ªÉ suy ra t·ª´ Close/Last)
    
    2. **Volume** t∆∞∆°ng quan y·∫øu v·ªõi c√°c c·ªôt gi√°
       - ‚úÖ **Gi·ªØ:** `Volume` (th√¥ng tin ƒë·ªôc l·∫≠p, h·ªØu √≠ch)
       - Kh·ªëi l∆∞·ª£ng giao d·ªãch ph·∫£n √°nh m·ª©c ƒë·ªô quan t√¢m c·ªßa th·ªã tr∆∞·ªùng
    
    **K·∫øt qu·∫£ cu·ªëi c√πng:**
    - ‚úÖ Gi·ªØ: `Close/Last`, `Volume`
    - ‚ùå B·ªè: `Open`, `High`, `Low` (d∆∞ th·ª´a, t∆∞∆°ng quan cao)
    """)
    
    # ===== DIMENSIONALITY REDUCTION WITH PCA =====
    st.subheader("üéØ Gi·∫£m chi·ªÅu d·ªØ li·ªáu (PCA - Principal Component Analysis)")
    
    # Prepare data for PCA
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df[quantitative_cols])
    
    # Apply PCA with all components
    pca_full = PCA()
    pca_full.fit(X_scaled)
    
    # Show cumulative variance explained
    cumsum_var = np.cumsum(pca_full.explained_variance_ratio_)
    
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.plot(range(1, len(cumsum_var)+1), cumsum_var, 'bo-', linewidth=2, markersize=8)
    ax.axhline(y=0.95, color='r', linestyle='--', label='95% Variance')
    ax.axhline(y=0.90, color='orange', linestyle='--', label='90% Variance')
    ax.set_xlabel("Number of Principal Components")
    ax.set_ylabel("Cumulative Explained Variance")
    ax.set_title("Cumulative Explained Variance by PCA Components")
    ax.legend()
    ax.grid(True, alpha=0.3)
    st.pyplot(fig)
    
    # Apply PCA with 2 components
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)
    
    # Visualization
    fig, ax = plt.subplots(figsize=(10, 6))
    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=range(len(df)), 
                        cmap='viridis', alpha=0.6, s=50)
    ax.set_xlabel(f"PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)")
    ax.set_ylabel(f"PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)")
    ax.set_title("PCA Projection of Gold Stock Data (2D)")
    plt.colorbar(scatter, ax=ax, label='Time Index')
    st.pyplot(fig)
    
    st.write("### üìä PCA Chi ti·∫øt (PCA Detailed Results)")
    pca_stats = {
        "Component": [f"PC{i+1}" for i in range(len(pca.explained_variance_ratio_))],
        "Explained Variance %": [f"{v:.2%}" for v in pca.explained_variance_ratio_],
        "Cumulative Variance %": [f"{c:.2%}" for c in np.cumsum(pca.explained_variance_ratio_)]
    }
    st.dataframe(pd.DataFrame(pca_stats), use_container_width=True)
    
    st.write(f"""
    **Nh·∫≠n x√©t:**
    - PC1 explains: **{pca.explained_variance_ratio_[0]:.2%}** c·ªßa ph∆∞∆°ng sai
    - PC2 explains: **{pca.explained_variance_ratio_[1]:.2%}** c·ªßa ph∆∞∆°ng sai
    - T·ªïng c·ªông: **{sum(pca.explained_variance_ratio_):.2%}** ph∆∞∆°ng sai ƒë∆∞·ª£c gi·∫£i th√≠ch
    
    **K·∫øt lu·∫≠n:** 2 th√†nh ph·∫ßn ch√≠nh gi·∫£i th√≠ch **{sum(pca.explained_variance_ratio_):.2%}** ph∆∞∆°ng sai,
    t·ª©c l√† gi·∫£m ƒë∆∞·ª£c t·ª´ {len(quantitative_cols)} chi·ªÅu xu·ªëng 2 chi·ªÅu m√† v·∫´n gi·ªØ l·∫°i h·∫ßu h·∫øt th√¥ng tin.
    """)
    
    # Feature loadings
    st.write("### üîç ƒê√≥ng g√≥p c·ªßa t·ª´ng bi·∫øn v√†o PC (Feature Loadings)")
    loadings_df = pd.DataFrame(
        pca.components_.T,
        columns=[f"PC{i+1}" for i in range(len(pca.components_))],
        index=quantitative_cols
    )
    st.dataframe(loadings_df, use_container_width=True)

# ==========================================================
# TAB 5: B5 ‚Äì MACHINE LEARNING MODEL & INTERACTIVE VISUALIZATION
# ==========================================================
with tab5:
    st.header("B5 ‚Äì M√¥ h√¨nh ML & Tr·ª±c quan h√≥a t∆∞∆°ng t√°c (Model & Visualization)")
    
    st.info("‚ö†Ô∏è **L∆∞u √Ω:** M√¥ h√¨nh K-Means d∆∞·ªõi ƒë√¢y ch·ªâ mang t√≠nh minh h·ªça ƒë·ªÉ ph√¢n c·ª•m d·ªØ li·ªáu. M·ª•c ƒë√≠ch l√† l√†m r√µ c·∫•u tr√∫c d·ªØ li·ªáu, kh√¥ng ƒë√°nh gi√° cao hi·ªáu su·∫•t d·ª± b√°o.")
    
    # ===== KMEANS CLUSTERING =====
    st.subheader("üéØ Ph√¢n c·ª•m d·ªØ li·ªáu (K-Means Clustering)")
    
    k = st.slider(
        "Ch·ªçn s·ªë c·ª•m (Select number of clusters)",
        min_value=2,
        max_value=6,
        value=3,
        step=1
    )
    
    # Apply KMeans
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    df["Cluster"] = kmeans.fit_predict(df[quantitative_cols])
    
    # ===== VISUALIZATION 1: SCATTER PLOTS =====
    st.write("### üìä Bi·ªÉu ƒë·ªì ph√¢n c·ª•m (Cluster Scatter Plots)")
    col1, col2 = st.columns(2)
    
    with col1:
        fig, ax = plt.subplots(figsize=(10, 6))
        scatter = ax.scatter(df["Open"], df["Close/Last"], 
                           c=df["Cluster"], cmap='Set2', s=80, alpha=0.6, edgecolors='black', linewidth=0.5)
        ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
                  c='red', marker='X', s=300, edgecolors='black', linewidth=2,
                  label='Centroids', zorder=5)
        ax.set_xlabel("Open Price ($)", fontsize=11)
        ax.set_ylabel("Close Price ($)", fontsize=11)
        ax.set_title(f"K-Means Clustering (Open vs Close) - K={k}")
        ax.legend()
        cbar = plt.colorbar(scatter, ax=ax, label='Cluster')
        st.pyplot(fig)
    
    with col2:
        fig, ax = plt.subplots(figsize=(10, 6))
        scatter = ax.scatter(df["Low"], df["High"],
                           c=df["Cluster"], cmap='Set2', s=80, alpha=0.6, edgecolors='black', linewidth=0.5)
        ax.scatter(kmeans.cluster_centers_[:, 2], kmeans.cluster_centers_[:, 3],
                  c='red', marker='X', s=300, edgecolors='black', linewidth=2,
                  label='Centroids', zorder=5)
        ax.set_xlabel("Low Price ($)", fontsize=11)
        ax.set_ylabel("High Price ($)", fontsize=11)
        ax.set_title(f"K-Means Clustering (Low vs High) - K={k}")
        ax.legend()
        cbar = plt.colorbar(scatter, ax=ax, label='Cluster')
        st.pyplot(fig)
           

    
    # ===== VISUALIZATION 2: TIME SERIES WITH CLUSTERS =====
    st.write("### ‚è∞ Ph√¢n b·ªë c·ª•m theo th·ªùi gian (Cluster Distribution Over Time)")
    fig, ax = plt.subplots(figsize=(14, 6))
    colors = plt.cm.Set2(np.linspace(0, 1, k))
    for cluster in range(k):
        cluster_data = df[df["Cluster"] == cluster]
        ax.scatter(cluster_data["Date"], cluster_data["Close/Last"],
                  label=f"Cluster {cluster}", alpha=0.6, s=40, color=colors[cluster])
    ax.set_xlabel("Date", fontsize=11)
    ax.set_ylabel("Close Price ($)", fontsize=11)
    ax.set_title(f"Gold Price with K-Means Clusters (K={k})")
    ax.legend(loc='best')
    plt.xticks(rotation=45)
    st.pyplot(fig)
    
    # ===== CLUSTER STATISTICS =====
    st.write("### üìà Th·ªëng k√™ chi ti·∫øt t·ª´ng c·ª•m (Cluster Statistics)")
    cluster_stats = df.groupby("Cluster")[quantitative_cols].agg(['mean', 'min', 'max', 'std'])
    st.dataframe(cluster_stats, use_container_width=True)
    
    # ===== CLUSTER SIZES =====
    st.write("### üìä K√≠ch th∆∞·ªõc c·ª•m (Cluster Sizes)")
    cluster_sizes = df["Cluster"].value_counts().sort_index()
    
    col1, col2 = st.columns(2)
    with col1:
        fig, ax = plt.subplots(figsize=(8, 5))
        ax.bar(cluster_sizes.index, cluster_sizes.values, color=colors, edgecolor='black', linewidth=1.5)
        ax.set_xlabel("Cluster", fontsize=11)
        ax.set_ylabel("Number of Data Points", fontsize=11)
        ax.set_title(f"Cluster Size Distribution (K={k})")
        ax.set_xticks(range(k))
        for i, v in enumerate(cluster_sizes.values):
            ax.text(i, v + 5, str(v), ha='center', fontweight='bold')
        st.pyplot(fig)
    
    with col2:
        fig, ax = plt.subplots(figsize=(8, 5))
        ax.pie(cluster_sizes.values, labels=[f"Cluster {i}\n({v} points)" for i, v in enumerate(cluster_sizes.values)],
               colors=colors, autopct='%1.1f%%', startangle=90, explode=[0.05]*k)
        ax.set_title(f"Cluster Distribution Percentage (K={k})")
        st.pyplot(fig)
    
    # ===== CLUSTER CHARACTERISTICS =====
    st.write("### üîç ƒê·∫∑c ƒëi·ªÉm c·ªßa t·ª´ng c·ª•m (Cluster Characteristics)")
    for cluster in range(k):
        cluster_data = df[df["Cluster"] == cluster]
        st.write(f"**Cluster {cluster}:** {len(cluster_data)} ƒëi·ªÉm d·ªØ li·ªáu")
        
        char_text = f"- **Gi√° Close trung b√¨nh:** ${cluster_data['Close/Last'].mean():.2f} (Range: ${cluster_data['Close/Last'].min():.2f} - ${cluster_data['Close/Last'].max():.2f})\n"
        char_text += f"- **Kh·ªëi l∆∞·ª£ng trung b√¨nh:** {cluster_data['Volume'].mean():.0f}\n"
        char_text += f"- **Giai ƒëo·∫°n th·ªùi gian:** {cluster_data['Date'].min().date()} ‚Üí {cluster_data['Date'].max().date()}"
        st.write(char_text)
    
    # ===== SAMPLE DATA WITH CLUSTERS =====
    st.write("### üìù D·ªØ li·ªáu m·∫´u sau ph√¢n c·ª•m (Sample Data with Clusters)")
    display_cols = ["Date", "Open", "High", "Low", "Close/Last", "Volume", "Cluster"]
    st.dataframe(df[display_cols].head(20), use_container_width=True)
    
    # ===== MODEL EVALUATION =====
    st.write("### üìä ƒê√°nh gi√° m√¥ h√¨nh K-Means (Model Evaluation)")
    
    # Calculate inertia and silhouette score
    inertia = kmeans.inertia_
    silhouette = silhouette_score(df[quantitative_cols], df["Cluster"])
    davies_bouldin = davies_bouldin_score(df[quantitative_cols], df["Cluster"])
    calinski = calinski_harabasz_score(df[quantitative_cols], df["Cluster"])
    
    eval_metrics = {
        "Metric": ["Inertia", "Silhouette Score", "Davies-Bouldin Index", "Calinski-Harabasz Index"],
        "Value": [
            f"{inertia:.2f}",
            f"{silhouette:.4f}",
            f"{davies_bouldin:.4f}",
            f"{calinski:.2f}"
        ],
        "Interpretation": [
            "Sum of squared distances (Lower is better)",
            "Clustering quality (-1 to 1, Higher is better)",
            "Cluster separation (Lower is better)",
            "Cluster density (Higher is better)"
        ]
    }
    st.dataframe(pd.DataFrame(eval_metrics), use_container_width=True)
    
    st.write(f"""
    **Nh·∫≠n x√©t:**
    - **Silhouette Score = {silhouette:.4f}**: {'T·ªët ‚úì' if silhouette > 0.5 else 'Trung b√¨nh ‚ö†' if silhouette > 0.3 else 'C·∫ßn c·∫£i thi·ªán ‚úó'}
    - M√¥ h√¨nh ph√¢n c·ª•m gi√∫p hi·ªÉu r√µ c·∫•u tr√∫c d·ªØ li·ªáu gi√° v√†ng
    - C√°c c·ª•m c√≥ th·ªÉ ƒë·∫°i di·ªán cho c√°c giai ƒëo·∫°n ho·∫∑c xu h∆∞·ªõng gi√° kh√°c nhau
    """)
    
    # ===========================
    # LINEAR REGRESSION - FORECAST (N√ÇNG C·∫§P ‚úÖ)
    # ===========================
    st.markdown("---")
    st.subheader("üìà D·ª± ƒëo√°n gi√° v√†ng (Linear Regression + Regularization)")
    
    # 1) Ch·ªçn horizon + t·ª∑ l·ªá test
    col1, col2, col3 = st.columns(3)
    with col1:
        horizon = st.selectbox("Ch·ªçn b∆∞·ªõc d·ª± ƒëo√°n (t+h, theo NG√ÄY)", [1, 7, 14, 30, 60, 90, 180, 252], index=3)
    with col2:
        test_ratio = st.slider("T·ª∑ l·ªá test (time split)", 0.15, 0.35, 0.25, 0.05)
    with col3:
        reg_type = st.radio("Regularization:", ["Ridge (L2)", "Lasso (L1)", "ElasticNet"], index=0)
    
    # 2) T·∫°o dataset supervised
    lr_data, feat_cols = make_lr_dataset(df, horizon=horizon)
    
    # ===== L·ªåC FEATURES NHI·ªÑU =====
    feat_corr = lr_data[feat_cols + ["y"]].corr()["y"].drop("y")
    strong_features = feat_corr[feat_corr.abs() > 0.01].index.tolist()
    strong_features = [f for f in strong_features if f in feat_cols]
    
    if len(strong_features) < 3:
        st.warning("Qu√° √≠t feature sau l·ªçc, d√πng t·∫•t c·∫£ features")
        feat_cols_selected = feat_cols
    else:
        feat_cols_selected = strong_features
    
    st.write(f"**Features sau l·ªçc:** {len(feat_cols_selected)}/{len(feat_cols)} (lo·∫°i b·ªè noise)")
    
    # Time split
    train_df, test_df = time_split(lr_data, test_ratio=test_ratio)
    X_train, y_train = train_df[feat_cols_selected], train_df["y"]
    X_test, y_test = test_df[feat_cols_selected], test_df["y"]
    
    # ===== CHU·∫®N H√ìA D·ªÆ LI·ªÜU =====
    scaler_robust = RobustScaler()
    X_train_scaled = scaler_robust.fit_transform(X_train)
    X_test_scaled = scaler_robust.transform(X_test)
    
    # ===== REGULARIZATION + TRAINING =====
    alpha = st.slider("ƒê·ªô m·∫°nh regularization (Œ±)", 0.001, 1.0, 0.1, 0.01)
    
    if reg_type == "Ridge (L2)":
        lr = Ridge(alpha=alpha)
    elif reg_type == "Lasso (L1)":
        lr = Lasso(alpha=alpha, max_iter=10000)
    else:
        lr = ElasticNet(alpha=alpha, l1_ratio=0.5, max_iter=10000)
    
    lr.fit(X_train_scaled, y_train)
    y_pred_train = lr.predict(X_train_scaled)
    y_pred = lr.predict(X_test_scaled)
    
    # ===== METRICS =====
    mae = mean_absolute_error(y_test, y_pred)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    r2 = lr.score(X_test_scaled, y_test)
    
    c1, c2, c3, c4, c5 = st.columns(5)
    c1.metric("MAE", f"{mae:.4f}")
    c2.metric("RMSE", f"{rmse:.4f}")
    c3.metric("R¬≤ Score", f"{r2:.4f}")
    c4.metric("Test size", f"{len(test_df)}")
    c5.metric("Horizon", f"t+{horizon}d")
    
    # ===== ACTUAL vs PREDICTED =====
    st.write("### üìä Actual vs Predicted Return (tr√™n t·∫≠p test)")
    
    fig, ax = plt.subplots(figsize=(13, 5))
    ax.plot(test_df["Date"].values, y_test.values, label="Actual Return", linewidth=2, color='blue')
    ax.plot(test_df["Date"].values, y_pred, label="Predicted Return", linewidth=2, color='red', alpha=0.7)
    ax.fill_between(test_df["Date"].values, y_test.values, y_pred, alpha=0.2, color='gray')
    ax.set_title(f"Actual vs Predicted Return (t+{horizon} ng√†y)")
    ax.set_xlabel("Date")
    ax.set_ylabel("Return (% change)")
    ax.legend()
    plt.xticks(rotation=45)
    st.pyplot(fig)
    
    # ===== RESIDUAL ANALYSIS =====
    st.write("### üîç Ph√¢n t√≠ch sai s·ªë (Residuals)")
    
    residuals = y_test - y_pred
    
    fig, axes = plt.subplots(1, 2, figsize=(13, 4))
    
    axes[0].hist(residuals, bins=30, color='skyblue', edgecolor='black')
    axes[0].set_title("Distribution of Residuals")
    axes[0].set_xlabel("Residual (Actual - Predicted)")
    axes[0].axvline(0, color='red', linestyle='--', linewidth=2)
    
    axes[1].scatter(y_pred, residuals, alpha=0.6, s=30)
    axes[1].axhline(0, color='red', linestyle='--', linewidth=2)
    axes[1].set_title("Residual Plot")
    axes[1].set_xlabel("Predicted Return")
    axes[1].set_ylabel("Residual")
    
    st.pyplot(fig)
    
    residual_mean = residuals.mean()
    residual_std = residuals.std()
    st.write(f"**Residual Mean:** {residual_mean:.6f} (n√™n ‚âà 0)")
    st.write(f"**Residual Std:** {residual_std:.6f}")
    
    # ===== FEATURE COEFFICIENTS =====
    st.write("### üß† ƒê√≥ng g√≥p c·ªßa feature (Top 15 Coefficients)")
    coef_df = pd.DataFrame({"Feature": feat_cols_selected, "Coef": lr.coef_})
    coef_df["AbsCoef"] = coef_df["Coef"].abs()
    coef_df = coef_df.sort_values("AbsCoef", ascending=False)
    
    st.dataframe(coef_df.drop(columns=["AbsCoef"]).head(15), use_container_width=True)
    
    # ===========================
    # FORECAST T∆Ø∆†NG LAI (N√ÇNG C·∫§P) ‚úÖ‚úÖ‚úÖ
    # ===========================
    st.markdown("---")
    st.subheader("üîÆ D·ª± b√°o t∆∞∆°ng lai (252 ng√†y, v·ªõi kho·∫£ng tin c·∫≠y)")
    
    do_forecast = st.checkbox("B·∫≠t d·ª± b√°o t∆∞∆°ng lai (multi-step forecast)", value=True)
    
    if do_forecast:
        col1, col2, col3 = st.columns(3)
        with col1:
            n_steps = st.slider("S·ªë ng√†y d·ª± b√°o", 30, 252, 90, 5)
        with col2:
            confidence_level = st.select_slider("Kho·∫£ng tin c·∫≠y", [0.68, 0.85, 0.95], value=0.95)
        with col3:
            use_business_days = st.checkbox("D√πng Business days", value=True)
        
        # ===== H√ÄM CH√çNH: T·∫†YO FEATURES T·ª™ L·ªäCH S·ª¨ =====
        def build_features_from_history_scaled(hist_df: pd.DataFrame, scaler_obj, feat_cols_list) -> np.ndarray:
            """X√¢y d·ª±ng 1 d√≤ng feature t·ª´ l·ªãch s·ª≠, tr·∫£ v·ªÅ array ƒë√£ chu·∫©n h√≥a"""
            row = {}
            
            # lags
            for lag in [1, 2, 3, 5, 7, 14, 30]:
                if len(hist_df) >= lag:
                    row[f"close_lag_{lag}"] = hist_df["Close/Last"].iloc[-lag]
                else:
                    row[f"close_lag_{lag}"] = np.nan
            
            # rolling
            row["ma_7"] = hist_df["Close/Last"].rolling(7).mean().iloc[-1] if len(hist_df) >= 7 else np.nan
            row["ma_14"] = hist_df["Close/Last"].rolling(14).mean().iloc[-1] if len(hist_df) >= 14 else np.nan
            row["ma_20"] = hist_df["Close/Last"].rolling(20).mean().iloc[-1] if len(hist_df) >= 20 else np.nan
            row["ma_30"] = hist_df["Close/Last"].rolling(30).mean().iloc[-1] if len(hist_df) >= 30 else np.nan
            row["ma_60"] = hist_df["Close/Last"].rolling(60).mean().iloc[-1] if len(hist_df) >= 60 else np.nan
            row["ma_200"] = hist_df["Close/Last"].rolling(200).mean().iloc[-1] if len(hist_df) >= 200 else np.nan
            
            row["std_7"] = hist_df["Close/Last"].rolling(7).std().iloc[-1] if len(hist_df) >= 7 else np.nan
            row["std_14"] = hist_df["Close/Last"].rolling(14).std().iloc[-1] if len(hist_df) >= 14 else np.nan
            row["std_20"] = hist_df["Close/Last"].rolling(20).std().iloc[-1] if len(hist_df) >= 20 else np.nan
            
            # momentum
            row["momentum_7"] = hist_df["Close/Last"].iloc[-1] - hist_df["Close/Last"].iloc[-8] if len(hist_df) >= 8 else np.nan
            row["momentum_14"] = hist_df["Close/Last"].iloc[-1] - hist_df["Close/Last"].iloc[-15] if len(hist_df) >= 15 else np.nan
            row["momentum_30"] = hist_df["Close/Last"].iloc[-1] - hist_df["Close/Last"].iloc[-31] if len(hist_df) >= 31 else np.nan
            
            # ROC
            row["roc_7"] = (hist_df["Close/Last"].iloc[-1] - hist_df["Close/Last"].iloc[-8]) / hist_df["Close/Last"].iloc[-8] if len(hist_df) >= 8 else np.nan
            row["roc_14"] = (hist_df["Close/Last"].iloc[-1] - hist_df["Close/Last"].iloc[-15]) / hist_df["Close/Last"].iloc[-15] if len(hist_df) >= 15 else np.nan
            row["roc_30"] = (hist_df["Close/Last"].iloc[-1] - hist_df["Close/Last"].iloc[-31]) / hist_df["Close/Last"].iloc[-31] if len(hist_df) >= 31 else np.nan
            
            # RSI
            delta = hist_df["Close/Last"].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs))
            row["rsi_14"] = rsi.iloc[-1] if len(rsi) > 0 else np.nan
            
            # Bollinger Bands
            ma_20 = hist_df["Close/Last"].rolling(20).mean().iloc[-1] if len(hist_df) >= 20 else np.nan
            std_20 = hist_df["Close/Last"].rolling(20).std().iloc[-1] if len(hist_df) >= 20 else np.nan
            if pd.notna(ma_20) and pd.notna(std_20):
                bb_upper = ma_20 + 2 * std_20
                bb_lower = ma_20 - 2 * std_20
                row["bb_position"] = (hist_df["Close/Last"].iloc[-1] - bb_lower) / (bb_upper - bb_lower)
            else:
                row["bb_position"] = np.nan
            row["bb_upper"] = bb_upper if pd.notna(bb_upper) else np.nan
            row["bb_lower"] = bb_lower if pd.notna(bb_lower) else np.nan
            
            # Volume
            if "Volume" in hist_df.columns:
                row["vol_lag_1"] = hist_df["Volume"].iloc[-1]
                row["vol_ma_7"] = hist_df["Volume"].rolling(7).mean().iloc[-1] if len(hist_df) >= 7 else np.nan
                row["vol_ma_30"] = hist_df["Volume"].rolling(30).mean().iloc[-1] if len(hist_df) >= 30 else np.nan
                row["vol_std"] = hist_df["Volume"].rolling(7).std().iloc[-1] if len(hist_df) >= 7 else np.nan
                row["price_vol_trend"] = (hist_df["Close/Last"].iloc[-1] - hist_df["Close/Last"].iloc[-2]) / hist_df["Close/Last"].iloc[-2] * hist_df["Volume"].iloc[-1] if len(hist_df) >= 2 else np.nan
            
            # Chuy·ªÉn th√†nh DataFrame, ch·ªçn c·ªôt c·∫ßn thi·∫øt, chu·∫©n h√≥a
            df_row = pd.DataFrame([row])
            df_row = df_row[feat_cols_list]
            
            x_scaled = scaler_obj.transform(df_row)
            return x_scaled.flatten()
        
        # ===== RECURSIVE FORECAST =====
        hist = df[["Date", "Close/Last"] + (["Volume"] if "Volume" in df.columns else [])].copy()
        hist = hist.sort_values("Date").reset_index(drop=True)
        
        last_date = hist["Date"].iloc[-1]
        last_close = hist["Close/Last"].iloc[-1]
        
        if use_business_days:
            future_dates = pd.bdate_range(last_date + pd.Timedelta(days=1), periods=n_steps)
        else:
            future_dates = pd.date_range(last_date + pd.Timedelta(days=1), periods=n_steps, freq="D")
        
        preds = []
        preds_std = []
        temp_hist = hist.copy()
        
        sigma = residual_std
        
        for i, dt in enumerate(future_dates):
            try:
                x_next = build_features_from_history_scaled(temp_hist, scaler_robust, feat_cols_selected)
                
                if np.isnan(x_next).any():
                    preds.append(np.nan)
                    preds_std.append(np.nan)
                else:
                    y_next = lr.predict(x_next.reshape(1, -1))[0]
                    preds.append(y_next)
                    preds_std.append(sigma)
                    
                    # C·∫≠p nh·∫≠t l·ªãch s·ª≠
                    new_close = temp_hist["Close/Last"].iloc[-1] * (1 + y_next)
                    new_row = {"Date": pd.Timestamp(dt), "Close/Last": new_close}
                    if "Volume" in temp_hist.columns:
                        new_row["Volume"] = temp_hist["Volume"].iloc[-1]
                    temp_hist = pd.concat([temp_hist, pd.DataFrame([new_row])], ignore_index=True)
            except Exception as e:
                st.warning(f"L·ªói t·∫°i b∆∞·ªõc {i}: {e}")
                preds.append(np.nan)
                preds_std.append(np.nan)
        
        forecast_df = pd.DataFrame({
            "Date": future_dates,
            "Forecast_Return": preds,
            "Forecast_Std": preds_std
        })
        forecast_df = forecast_df.dropna().reset_index(drop=True)
        
        # T√≠nh Confidence Interval
        z_score = stats.norm.ppf((1 + confidence_level) / 2)

# ==========================================================
# DATA MINING PROJECT - GOLD PRICE DATA
# Dataset: goldstock v2.csv
# B1 ‚Üí B5 (EDA-focused, model minh h·ªça, c√≥ GUI)
# ==========================================================

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import streamlit as st

from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.metrics import mean_absolute_error, mean_squared_error, silhouette_score, davies_bouldin_score, calinski_harabasz_score
from scipy import stats


plt.style.use('ggplot')

# ==========================================================
# LOAD DATA - B0: DATASET OVERVIEW
# ==========================================================
original_df = pd.read_csv("goldstock v2.csv")

# X√≥a c·ªôt index kh√¥ng c·∫ßn thi·∫øt
if "Unnamed: 0" in original_df.columns:
    original_df.drop(columns=["Unnamed: 0"], inplace=True)

# Chuy·ªÉn Date sang datetime
original_df["Date"] = pd.to_datetime(original_df["Date"])

# S·∫Øp x·∫øp theo th·ªùi gian
original_df.sort_values(by="Date", inplace=True)
original_df.reset_index(drop=True, inplace=True)
# ===== Convert c√°c c·ªôt s·ªë v·ªÅ numeric (ph√≤ng tr∆∞·ªùng h·ª£p c√≥ '$' v√† ',' ) =====
def to_numeric_clean(s: pd.Series):
    if s.dtype == "O":  # object/string
        s = (s.astype(str)
               .str.replace("$", "", regex=False)
               .str.replace(",", "", regex=False)
               .str.strip())
    return pd.to_numeric(s, errors="coerce")

for col in ["Open", "High", "Low", "Close/Last", "Volume"]:
    if col in original_df.columns:
        original_df[col] = to_numeric_clean(original_df[col])

# ===== CLEANING B·ªî SUNG SAU CONVERT NUMERIC =====

# Drop Date l·ªói
original_df = original_df.dropna(subset=["Date"]).copy()

# Drop NaN ph√°t sinh sau khi convert numeric
num_cols = ["Open", "High", "Low", "Close/Last", "Volume"]
num_cols = [c for c in num_cols if c in original_df.columns]
original_df = original_df.dropna(subset=num_cols).copy()

# Volume ph·∫£i kh√¥ng √¢m
if "Volume" in original_df.columns:
    original_df = original_df[original_df["Volume"] >= 0].copy()


# ==========================================================
# B2 ‚Äì DATA CLEANING (TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU)
# ==========================================================
# Clone m·ªôt b·∫£n sao ƒë·ªÉ gi·ªØ nguy√™n d·ªØ li·ªáu g·ªëc
df = original_df.copy()

# Ki·ªÉm tra d·ªØ li·ªáu thi·∫øu
missing_data = df.isnull().sum() * 100 / df.shape[0]

# X√≥a duplicate
# Remove duplicate theo Date (chu·∫©n time-series)
df = df.sort_values("Date").drop_duplicates(subset=["Date"], keep="last").reset_index(drop=True)


# Ki·ªÉm tra logic gi√° (High >= Open, Close, Low; Low <= Open, Close)
df = df[
    (df["High"] >= df["Open"]) &
    (df["High"] >= df["Close/Last"]) &
    (df["High"] >= df["Low"]) &
    (df["Low"] <= df["Open"]) &
    (df["Low"] <= df["Close/Last"])
]
# ===== VALIDATION B·ªî SUNG =====

# Gi√° ph·∫£i d∆∞∆°ng
for c in ["Open", "High", "Low", "Close/Last"]:
    df = df[df[c] > 0]

# Close v√† Open ph·∫£i n·∫±m trong [Low, High]
df = df[
    (df["Close/Last"] >= df["Low"]) &
    (df["Close/Last"] <= df["High"]) &
    (df["Open"] >= df["Low"]) &
    (df["Open"] <= df["High"])
]

df.reset_index(drop=True, inplace=True)

df.reset_index(drop=True, inplace=True)

# ==========================================================
# B1 ‚Äì M√î T·∫¢ D·ªÆ LI·ªÜU (DATA OVERVIEW)
# ==========================================================
quantitative_cols = df.select_dtypes(include=["int64", "float64"]).columns.tolist()
qualitative_cols = df.select_dtypes(exclude=["int64", "float64"]).columns.tolist()

# ==========================================================
# B5+ (SUPERVISED) ‚Äì LINEAR REGRESSION FORECAST (N√ÇNG C·∫§P)
# ==========================================================
def make_lr_dataset(df_in: pd.DataFrame, horizon: int = 1):
    """
    horizon=1: d·ª± ƒëo√°n Close(t+1)
    horizon=7: d·ª± ƒëo√°n Close(t+7)
    
    ‚úÖ C·∫¢I THI·ªÜN: Th√™m nhi·ªÅu feature ch·∫•t l∆∞·ª£ng cao (momentum, ROC, RSI, Bollinger Bands)
    """
    d = df_in.copy()

    # Target: future return (thay v√¨ absolute price - d·ªÖ scale h∆°n)
    d["y"] = d["Close/Last"].shift(-horizon) / d["Close/Last"] - 1

    # ===== LAG FEATURES (gi·ªØ nguy√™n) =====
    for lag in [1, 2, 3, 5, 7, 14, 30]:
        d[f"close_lag_{lag}"] = d["Close/Last"].shift(lag)

    # ===== ROLLING FEATURES (N√ÇNG C·∫§P) =====
    d["ma_7"] = d["Close/Last"].rolling(7).mean()
    d["ma_14"] = d["Close/Last"].rolling(14).mean()
    d["ma_20"] = d["Close/Last"].rolling(20).mean()
    d["ma_30"] = d["Close/Last"].rolling(30).mean()
    d["ma_60"] = d["Close/Last"].rolling(60).mean()
    d["ma_200"] = d["Close/Last"].rolling(200).mean()
    
    d["std_7"] = d["Close/Last"].rolling(7).std()
    d["std_14"] = d["Close/Last"].rolling(14).std()
    d["std_20"] = d["Close/Last"].rolling(20).std()
    
    # ===== MOMENTUM & TREND FEATURES (T·ªêI QUAN TR·ªåNG - C·∫¢I THI·ªÜN) =====
    d["momentum_7"] = d["Close/Last"] - d["Close/Last"].shift(7)
    d["momentum_14"] = d["Close/Last"] - d["Close/Last"].shift(14)
    d["momentum_30"] = d["Close/Last"] - d["Close/Last"].shift(30)
    
    # Rate of change (t·ªëc ƒë·ªô thay ƒë·ªïi %)
    d["roc_7"] = (d["Close/Last"] - d["Close/Last"].shift(7)) / d["Close/Last"].shift(7)
    d["roc_14"] = (d["Close/Last"] - d["Close/Last"].shift(14)) / d["Close/Last"].shift(14)
    d["roc_30"] = (d["Close/Last"] - d["Close/Last"].shift(30)) / d["Close/Last"].shift(30)
    
    # RSI-like oscillator (0-100)
    delta = d["Close/Last"].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    d["rsi_14"] = 100 - (100 / (1 + rs))
    
    # Bollinger Bands (ƒë·ªô l·ªách so v·ªõi MA)
    d["bb_upper"] = d["ma_20"] + 2 * d["std_20"]
    d["bb_lower"] = d["ma_20"] - 2 * d["std_20"]
    d["bb_position"] = (d["Close/Last"] - d["bb_lower"]) / (d["bb_upper"] - d["bb_lower"])
    
    # ===== VOLUME FEATURES (C·∫¨P NH·∫¨T) =====
    if "Volume" in d.columns:
        d["vol_lag_1"] = d["Volume"].shift(1)
        d["vol_ma_7"] = d["Volume"].rolling(7).mean()
        d["vol_ma_30"] = d["Volume"].rolling(30).mean()
        d["vol_std"] = d["Volume"].rolling(7).std()
        
        # Volume-Price Trend
        d["price_vol_trend"] = (d["Close/Last"] - d["Close/Last"].shift(1)) / d["Close/Last"].shift(1) * d["Volume"]

    # ===== LO·∫†I B·ªé NaN & CH·ªåN FEATURES =====
    d = d.dropna().reset_index(drop=True)

    feature_cols = [c for c in d.columns if c.startswith(("close_lag_", "ma_", "std_", "vol_", "momentum_", "roc_", "rsi_", "bb_", "price_"))]
    return d, feature_cols


def time_split(d: pd.DataFrame, test_ratio: float = 0.2):
    n = len(d)
    test_n = max(1, int(n * test_ratio))
    train = d.iloc[:-test_n].copy()
    test = d.iloc[-test_n:].copy()
    return train, test


# ...existing code...

# ==========================================================
# STREAMLIT GUI CONFIGURATION
# ==========================================================
st.set_page_config(page_title="Gold Price Data Mining", layout="wide")
st.title("üìä Gold Price Data Mining Project")
st.markdown("---")

# Create tabs for each phase
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "B1 - Data Overview",
    "B2 - Data Cleaning",
    "B3 - Exploratory Analysis",
    "B4 - Correlation & Dimensionality",
    "B5 - Model & Visualization"
])

# ==========================================================
# TAB 1: B1 ‚Äì DATA OVERVIEW (M√î T·∫¢ D·ªÆ LI·ªÜU)
# ==========================================================
with tab1:
    st.header("B1 ‚Äì M√¥ t·∫£ d·ªØ li·ªáu (Dataset Overview)")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total Rows", df.shape[0])
    with col2:
        st.metric("Total Columns", df.shape[1])
    with col3:
        st.metric("Date Range", f"{df['Date'].min().date()} to {df['Date'].max().date()}")
    
    st.write("### üìã Danh s√°ch c√°c c·ªôt (Dataset Columns)")
    st.write(df.columns.tolist())
    
    st.write("### üìä Ph√¢n lo·∫°i d·ªØ li·ªáu (Data Types Classification)")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**ƒê·ªãnh l∆∞·ª£ng (Quantitative):**")
        st.write(f"- S·ªë c·ªôt: **{len(quantitative_cols)}**")
        for col in quantitative_cols:
            st.write(f"  - `{col}` ({df[col].dtype})")
    
    with col2:
        st.write("**ƒê·ªãnh t√≠nh (Qualitative):**")
        st.write(f"- S·ªë c·ªôt: **{len(qualitative_cols)}**")
        for col in qualitative_cols:
            st.write(f"  - `{col}` ({df[col].dtype})")
    
    st.write("### üìà Th·ªëng k√™ m√¥ t·∫£ chi ti·∫øt (Descriptive Statistics)")
    st.dataframe(df[quantitative_cols].describe(), use_container_width=True)
    
    st.write("### üîç Th√¥ng tin chi ti·∫øt c√°c c·ªôt (Detailed Column Info)")
    info_data = {
        "Column": df.columns,
        "Data Type": df.dtypes.astype(str),
        "Non-Null Count": df.count(),
        "Null Count": df.isnull().sum(),
        "Min": [df[col].min() if col in quantitative_cols else "N/A" for col in df.columns],
        "Max": [df[col].max() if col in quantitative_cols else "N/A" for col in df.columns],
    }
    st.dataframe(pd.DataFrame(info_data), use_container_width=True)
    
    st.write("### üìù D·ªØ li·ªáu m·∫´u (Sample Data)")
    st.dataframe(df.head(10), use_container_width=True)

# ==========================================================
# TAB 2: B2 ‚Äì DATA CLEANING (TI·ªÄN X·ª¨ L√ù)
# ==========================================================
with tab2:
    st.header("B2 ‚Äì Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu (Data Cleaning)")
    
    st.write("### üîç 1. Ki·ªÉm tra d·ªØ li·ªáu thi·∫øu (Missing Data)")
    missing_count = original_df.isnull().sum()
    missing_percent = (missing_count / len(original_df)) * 100
    missing_df = pd.DataFrame({
        "Column": missing_count.index,
        "Missing Count": missing_count.values,
        "Missing %": missing_percent.values
    })
    st.dataframe(missing_df[missing_df["Missing Count"] > 0] if missing_df["Missing Count"].sum() > 0 
                 else pd.DataFrame({"Status": ["‚úÖ No missing data found"]}), use_container_width=True)
    
    st.write("### üîÑ 2. Ki·ªÉm tra d·ªØ li·ªáu tr√πng l·∫∑p (Duplicate Data)")
    col1, col2 = st.columns(2)
    with col1:
        st.write(f"‚ùå **Tr∆∞·ªõc x·ª≠ l√Ω:** {original_df.duplicated().sum()} d√≤ng tr√πng l·∫∑p")
    with col2:
        st.write(f"‚úÖ **Sau x·ª≠ l√Ω:** {df.duplicated().sum()} d√≤ng tr√πng l·∫∑p")
    
    st.write("### üö® 3. Ph√°t hi·ªán Noise & Outliers")
    
    st.write("### üìå Outlier theo Return (khuy·∫øn ngh·ªã cho d·ªØ li·ªáu t√†i ch√≠nh)")

    # 1) Check column
    if "Close/Last" not in df.columns:
        st.error("Kh√¥ng t√¨m th·∫•y c·ªôt 'Close/Last' trong df.")
    else:
        # 2) √âp ki·ªÉu numeric an to√†n
        close = pd.to_numeric(df["Close/Last"], errors="coerce")

        # 3) T√≠nh return + l√†m s·∫°ch NaN/Inf
        ret = close.pct_change()
        ret = ret.replace([np.inf, -np.inf], np.nan).dropna()

        # 4) N·∫øu d·ªØ li·ªáu qu√° √≠t th√¨ c·∫£nh b√°o
        if len(ret) < 30:
            st.warning(f"Return h·ª£p l·ªá qu√° √≠t ({len(ret)} ƒëi·ªÉm) n√™n th·ªëng k√™ outlier kh√¥ng ƒë√°ng tin.")
        else:
            q1, q3 = ret.quantile(0.25), ret.quantile(0.75)
            iqr = q3 - q1
            lb, ub = q1 - 1.5 * iqr, q3 + 1.5 * iqr

            out_ret = ((ret < lb) | (ret > ub)).sum()
            st.write(f"- S·ªë outlier theo return (IQR): **{out_ret}**")
            st.write(f"- Ng∆∞·ª°ng IQR: **[{lb:.4f}, {ub:.4f}]**")

            st.info("Outlier gi√° KH√îNG n√™n x√≥a v√¨ ph·∫£n √°nh bi·∫øn ƒë·ªông th·ªã tr∆∞·ªùng. N·∫øu c·∫ßn ·ªïn ƒë·ªãnh m√¥ h√¨nh, c√¢n nh·∫Øc x·ª≠ l√Ω outlier tr√™n return/feature.")
        
        # Ki·ªÉm tra outliers b·∫±ng IQR
        st.write("**Ph∆∞∆°ng ph√°p IQR (Interquartile Range):**")
        outlier_summary = []
        
        for col in quantitative_cols:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outlier_count = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
            outlier_summary.append({
                "Column": col,
                "Q1": round(Q1, 2),
                "Q3": round(Q3, 2),
                "IQR": round(IQR, 2),
                "Lower Bound": round(lower_bound, 2),
                "Upper Bound": round(upper_bound, 2),
                "Outlier Count": outlier_count
            })
        
        outlier_df = pd.DataFrame(outlier_summary)
        st.dataframe(outlier_df, use_container_width=True)
        
        # Visualize outliers
        fig, axes = plt.subplots(len(quantitative_cols), 1, figsize=(10, 3*len(quantitative_cols)))
        if len(quantitative_cols) == 1:
            axes = [axes]
        
        for idx, col in enumerate(quantitative_cols):
            sns.boxplot(data=df, y=col, ax=axes[idx], color='steelblue')
            axes[idx].set_title(f"Outlier Detection: {col}")
        
        plt.tight_layout()
        st.pyplot(fig)
        
        st.write("### ‚úÖ 4. C√°c quy t·∫Øc x√°c th·ª±c d·ªØ li·ªáu (Data Validation Rules)")
        st.write("""
        ‚úîÔ∏è **High >= Open, Close/Last, Low** - Gi√° cao nh·∫•t >= t·∫•t c·∫£ c√°c m·ª©c gi√° kh√°c
        ‚úîÔ∏è **Low <= Open, Close/Last** - Gi√° th·∫•p nh·∫•t <= t·∫•t c·∫£ c√°c m·ª©c gi√° kh√°c
        ‚úîÔ∏è **Volume >= 0** - Kh·ªëi l∆∞·ª£ng kh√¥ng √¢m
        ‚úîÔ∏è **Date sorted in ascending order** - D·ªØ li·ªáu s·∫Øp x·∫øp theo th·ªùi gian
        ‚úîÔ∏è **Duplicates removed** - Lo·∫°i b·ªè d√≤ng tr√πng l·∫∑p
        """)
        
        st.write("### üìä 5. So s√°nh d·ªØ li·ªáu tr∆∞·ªõc & sau l√†m s·∫°ch")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Rows (Before)", original_df.shape[0])
        with col2:
            st.metric("Rows (After)", df.shape[0])
        with col3:
            st.metric("Rows Removed", original_df.shape[0] - df.shape[0])
        
        st.write("### üìù D·ªØ li·ªáu m·∫´u sau l√†m s·∫°ch (Sample Cleaned Data)")
        st.dataframe(df.head(10), use_container_width=True)

# ==========================================================
# TAB 3: B3 ‚Äì EXPLORATORY DATA ANALYSIS (KHAI PH√Å D·ªÆ LI·ªÜU)
# ==========================================================
with tab3:
    st.header("B3 ‚Äì Khai ph√° d·ªØ li·ªáu (Exploratory Data Analysis)")
    
    # ===== UNIVARIATE ANALYSIS =====
    st.subheader("üìä Ph√¢n t√≠ch ƒë∆°n bi·∫øn (Univariate Analysis)")
    
    col_univariate = st.selectbox(
        "Ch·ªçn c·ªôt ƒë·ªãnh l∆∞·ª£ng ƒë·ªÉ ph√¢n t√≠ch",
        quantitative_cols,
        key="univariate"
    )
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Histogram with KDE
        fig, ax = plt.subplots(figsize=(10, 5))
        sns.histplot(df[col_univariate], kde=True, ax=ax, color='steelblue', bins=30)
        ax.set_title(f"Distribution of {col_univariate}")
        ax.set_xlabel(col_univariate)
        ax.set_ylabel("Frequency")
        st.pyplot(fig)
    
    with col2:
        # Box plot
        fig, ax = plt.subplots(figsize=(10, 5))
        sns.boxplot(data=df, y=col_univariate, ax=ax, color='lightcoral')
        ax.set_title(f"Box Plot of {col_univariate}")
        st.pyplot(fig)
    
    # Statistical summary
    st.write(f"### üìà Th·ªëng k√™ m√¥ t·∫£: {col_univariate}")
    stats_data = {
        "Metric": ["Count", "Mean", "Std Dev", "Min", "25%", "Median", "75%", "Max", "Range", "Skewness"],
        "Value": [
            df[col_univariate].count(),
            round(df[col_univariate].mean(), 2),
            round(df[col_univariate].std(), 2),
            round(df[col_univariate].min(), 2),
            round(df[col_univariate].quantile(0.25), 2),
            round(df[col_univariate].median(), 2),
            round(df[col_univariate].quantile(0.75), 2),
            round(df[col_univariate].max(), 2),
            round(df[col_univariate].max() - df[col_univariate].min(), 2),
            round(df[col_univariate].skew(), 2),
        ]
    }
    st.dataframe(pd.DataFrame(stats_data), use_container_width=True)
    
    # ===== BIVARIATE ANALYSIS =====
    st.subheader("üìà Ph√¢n t√≠ch ƒëa bi·∫øn (Bivariate Analysis)")
    
    col1, col2 = st.columns(2)
    with col1:
        x_col = st.selectbox("Ch·ªçn tr·ª•c X", quantitative_cols, index=0, key="x_axis")
    with col2:
        y_col = st.selectbox("Ch·ªçn tr·ª•c Y", quantitative_cols, index=1 if len(quantitative_cols) > 1 else 0, key="y_axis")
    
    fig, ax = plt.subplots(figsize=(10, 6))
    sns.scatterplot(x=df[x_col], y=df[y_col], ax=ax, alpha=0.6, color='steelblue', s=50)
    # Th√™m trendline
    z = np.polyfit(df[x_col], df[y_col], 1)
    p = np.poly1d(z)
    ax.plot(df[x_col].sort_values(), p(df[x_col].sort_values()), "r--", linewidth=2, label='Trend')
    ax.set_title(f"Scatter Plot: {x_col} vs {y_col}")
    ax.legend()
    st.pyplot(fig)
    
    # Correlation coefficient
    corr_coef = df[x_col].corr(df[y_col])
    st.write(f"**H·ªá s·ªë t∆∞∆°ng quan Pearson:** {corr_coef:.4f}")
    
    # ===== TIME SERIES ANALYSIS =====
    st.subheader("‚è∞ Ph√¢n t√≠ch chu·ªói th·ªùi gian (Time Series Analysis)")
    
    fig, ax = plt.subplots(figsize=(12, 5))
    ax.plot(df["Date"], df["Close/Last"], linewidth=2, color='steelblue', label='Close Price')
    ax.fill_between(df["Date"], df["Low"], df["High"], alpha=0.2, color='lightblue', label='High-Low Range')
    ax.set_xlabel("Date")
    ax.set_ylabel("Price (USD/oz)")
    ax.set_title("Xu h∆∞·ªõng gi√° v√†ng (Gold Price Trend)")
    ax.legend()
    plt.xticks(rotation=45)
    st.pyplot(fig)
    
    # Th·ªëng k√™ chu·ªói th·ªùi gian
    st.write("### üìä Th·ªëng k√™ chu·ªói th·ªùi gian:")
    time_stats = {
        "Metric": ["Avg Close Price", "Max Close Price", "Min Close Price", "Price Range", "Volatility (Std)"],
        "Value": [
            f"${df['Close/Last'].mean():.2f}",
            f"${df['Close/Last'].max():.2f}",
            f"${df['Close/Last'].min():.2f}",
            f"${df['Close/Last'].max() - df['Close/Last'].min():.2f}",
            f"${df['Close/Last'].std():.2f}"
        ]
    }
    st.dataframe(pd.DataFrame(time_stats), use_container_width=True)
    
    # ===== VOLUME ANALYSIS =====
    st.subheader("üìä Ph√¢n t√≠ch kh·ªëi l∆∞·ª£ng giao d·ªãch (Volume Analysis)")
    
    fig, ax = plt.subplots(figsize=(12, 5))
    ax.bar(df["Date"], df["Volume"], color='steelblue', alpha=0.7, width=0.8)
    ax.set_xlabel("Date")
    ax.set_ylabel("Volume")
    ax.set_title("Kh·ªëi l∆∞·ª£ng giao d·ªãch (Trading Volume Over Time)")
    plt.xticks(rotation=45)
    st.pyplot(fig)
    
    # Volume statistics
    st.write("### üìä Th·ªëng k√™ kh·ªëi l∆∞·ª£ng:")
    volume_stats = {
        "Metric": ["Avg Volume", "Max Volume", "Min Volume", "Total Volume"],
        "Value": [
            f"{df['Volume'].mean():.0f}",
            f"{df['Volume'].max():.0f}",
            f"{df['Volume'].min():.0f}",
            f"{df['Volume'].sum():.0f}"
        ]
    }
    st.dataframe(pd.DataFrame(volume_stats), use_container_width=True)

# ==========================================================
# TAB 4: B4 ‚Äì CORRELATION & DIMENSIONALITY REDUCTION
# ==========================================================
with tab4:
    st.header("B4 ‚Äì Ma tr·∫≠n t∆∞∆°ng quan & Gi·∫£m chi·ªÅu (Correlation & PCA)")
    
    # ===== CORRELATION MATRIX =====
    st.subheader("üîó Ma tr·∫≠n t∆∞∆°ng quan (Correlation Matrix)")
    
    corr_matrix = df[quantitative_cols].corr()
    
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", center=0, 
                square=True, ax=ax, fmt=".2f", cbar_kws={'label': 'Correlation'})
    ax.set_title("Correlation Matrix of Quantitative Variables")
    st.pyplot(fig)
    
    st.write("### üß† Ph√¢n t√≠ch & L·∫≠p lu·∫≠n Gi·ªØ/B·ªè c·ªôt (Analysis & Column Selection Reasoning)")
    
    # T√≠nh t∆∞∆°ng quan cao
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > 0.95:
                high_corr_pairs.append({
                    "Column 1": corr_matrix.columns[i],
                    "Column 2": corr_matrix.columns[j],
                    "Correlation": round(corr_matrix.iloc[i, j], 4)
                })
    
    if high_corr_pairs:
        st.write("**C√°c c·ªôt c√≥ t∆∞∆°ng quan r·∫•t cao (> 0.95):**")
        st.dataframe(pd.DataFrame(high_corr_pairs), use_container_width=True)
    
    st.write("""
    **K·∫øt lu·∫≠n & Quy·∫øt ƒë·ªãnh:**
    
    1. **Open, High, Low, Close/Last** c√≥ t∆∞∆°ng quan r·∫•t cao (> 0.95)
       - ‚ùå Gi·ªØ t·∫•t c·∫£ 4 c·ªôt l√† d∆∞ th·ª´a
       - ‚úÖ **Gi·ªØ:** `Close/Last` (gi√° ƒë√≥ng c·ª≠a - ch·ªâ b√°o ch√≠nh)
       - ‚ùå **B·ªè:** `Open`, `High`, `Low` (c√≥ th·ªÉ suy ra t·ª´ Close/Last)
    
    2. **Volume** t∆∞∆°ng quan y·∫øu v·ªõi c√°c c·ªôt gi√°
       - ‚úÖ **Gi·ªØ:** `Volume` (th√¥ng tin ƒë·ªôc l·∫≠p, h·ªØu √≠ch)
       - Kh·ªëi l∆∞·ª£ng giao d·ªãch ph·∫£n √°nh m·ª©c ƒë·ªô quan t√¢m c·ªßa th·ªã tr∆∞·ªùng
    
    **K·∫øt qu·∫£ cu·ªëi c√πng:**
    - ‚úÖ Gi·ªØ: `Close/Last`, `Volume`
    - ‚ùå B·ªè: `Open`, `High`, `Low` (d∆∞ th·ª´a, t∆∞∆°ng quan cao)
    """)
    
    # ===== DIMENSIONALITY REDUCTION WITH PCA =====
    st.subheader("üéØ Gi·∫£m chi·ªÅu d·ªØ li·ªáu (PCA - Principal Component Analysis)")
    
    # Prepare data for PCA
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df[quantitative_cols])
    
    # Apply PCA with all components
    pca_full = PCA()
    pca_full.fit(X_scaled)
    
    # Show cumulative variance explained
    cumsum_var = np.cumsum(pca_full.explained_variance_ratio_)
    
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.plot(range(1, len(cumsum_var)+1), cumsum_var, 'bo-', linewidth=2, markersize=8)
    ax.axhline(y=0.95, color='r', linestyle='--', label='95% Variance')
    ax.axhline(y=0.90, color='orange', linestyle='--', label='90% Variance')
    ax.set_xlabel("Number of Principal Components")
    ax.set_ylabel("Cumulative Explained Variance")
    ax.set_title("Cumulative Explained Variance by PCA Components")
    ax.legend()
    ax.grid(True, alpha=0.3)
    st.pyplot(fig)
    
    # Apply PCA with 2 components
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)
    
    # Visualization
    fig, ax = plt.subplots(figsize=(10, 6))
    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=range(len(df)), 
                        cmap='viridis', alpha=0.6, s=50)
    ax.set_xlabel(f"PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)")
    ax.set_ylabel(f"PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)")
    ax.set_title("PCA Projection of Gold Stock Data (2D)")
    plt.colorbar(scatter, ax=ax, label='Time Index')
    st.pyplot(fig)
    
    st.write("### üìä PCA Chi ti·∫øt (PCA Detailed Results)")
    pca_stats = {
        "Component": [f"PC{i+1}" for i in range(len(pca.explained_variance_ratio_))],
        "Explained Variance %": [f"{v:.2%}" for v in pca.explained_variance_ratio_],
        "Cumulative Variance %": [f"{c:.2%}" for c in np.cumsum(pca.explained_variance_ratio_)]
    }
    st.dataframe(pd.DataFrame(pca_stats), use_container_width=True)
    
    st.write(f"""
    **Nh·∫≠n x√©t:**
    - PC1 explains: **{pca.explained_variance_ratio_[0]:.2%}** c·ªßa ph∆∞∆°ng sai
    - PC2 explains: **{pca.explained_variance_ratio_[1]:.2%}** c·ªßa ph∆∞∆°ng sai
    - T·ªïng c·ªông: **{sum(pca.explained_variance_ratio_):.2%}** ph∆∞∆°ng sai ƒë∆∞·ª£c gi·∫£i th√≠ch
    
    **K·∫øt lu·∫≠n:** 2 th√†nh ph·∫ßn ch√≠nh gi·∫£i th√≠ch **{sum(pca.explained_variance_ratio_):.2%}** ph∆∞∆°ng sai,
    t·ª©c l√† gi·∫£m ƒë∆∞·ª£c t·ª´ {len(quantitative_cols)} chi·ªÅu xu·ªëng 2 chi·ªÅu m√† v·∫´n gi·ªØ l·∫°i h·∫ßu h·∫øt th√¥ng tin.
    """)
    
    # Feature loadings
    st.write("### üîç ƒê√≥ng g√≥p c·ªßa t·ª´ng bi·∫øn v√†o PC (Feature Loadings)")
    loadings_df = pd.DataFrame(
        pca.components_.T,
        columns=[f"PC{i+1}" for i in range(len(pca.components_))],
        index=quantitative_cols
    )
    st.dataframe(loadings_df, use_container_width=True)

# ==========================================================
# TAB 5: B5 ‚Äì MACHINE LEARNING MODEL & INTERACTIVE VISUALIZATION
# ==========================================================
with tab5:
    st.header("B5 ‚Äì M√¥ h√¨nh ML & Tr·ª±c quan h√≥a t∆∞∆°ng t√°c (Model & Visualization)")
    
    st.info("‚ö†Ô∏è **L∆∞u √Ω:** M√¥ h√¨nh K-Means d∆∞·ªõi ƒë√¢y ch·ªâ mang t√≠nh minh h·ªça ƒë·ªÉ ph√¢n c·ª•m d·ªØ li·ªáu. M·ª•c ƒë√≠ch l√† l√†m r√µ c·∫•u tr√∫c d·ªØ li·ªáu, kh√¥ng ƒë√°nh gi√° cao hi·ªáu su·∫•t d·ª± b√°o.")
    
    # ===== KMEANS CLUSTERING =====
    st.subheader("üéØ Ph√¢n c·ª•m d·ªØ li·ªáu (K-Means Clustering)")
    
    k = st.slider(
        "Ch·ªçn s·ªë c·ª•m (Select number of clusters)",
        min_value=2,
        max_value=6,
        value=3,
        step=1
    )
    
    # Apply KMeans
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    df["Cluster"] = kmeans.fit_predict(df[quantitative_cols])
    
    # ===== VISUALIZATION 1: SCATTER PLOTS =====
    st.write("### üìä Bi·ªÉu ƒë·ªì ph√¢n c·ª•m (Cluster Scatter Plots)")
    col1, col2 = st.columns(2)
    
    with col1:
        fig, ax = plt.subplots(figsize=(10, 6))
        scatter = ax.scatter(df["Open"], df["Close/Last"], 
                           c=df["Cluster"], cmap='Set2', s=80, alpha=0.6, edgecolors='black', linewidth=0.5)
        ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
                  c='red', marker='X', s=300, edgecolors='black', linewidth=2,
                  label='Centroids', zorder=5)
        ax.set_xlabel("Open Price ($)", fontsize=11)
        ax.set_ylabel("Close Price ($)", fontsize=11)
        ax.set_title(f"K-Means Clustering (Open vs Close) - K={k}")
        ax.legend()
        cbar = plt.colorbar(scatter, ax=ax, label='Cluster')
        st.pyplot(fig)
    
    with col2:
        fig, ax = plt.subplots(figsize=(10, 6))
        scatter = ax.scatter(df["Low"], df["High"],
                           c=df["Cluster"], cmap='Set2', s=80, alpha=0.6, edgecolors='black', linewidth=0.5)
        ax.scatter(kmeans.cluster_centers_[:, 2], kmeans.cluster_centers_[:, 3],
                  c='red', marker='X', s=300, edgecolors='black', linewidth=2,
                  label='Centroids', zorder=5)
        ax.set_xlabel("Low Price ($)", fontsize=11)
        ax.set_ylabel("High Price ($)", fontsize=11)
        ax.set_title(f"K-Means Clustering (Low vs High) - K={k}")
        ax.legend()
        cbar = plt.colorbar(scatter, ax=ax, label='Cluster')
        st.pyplot(fig)
           

    
    # ===== VISUALIZATION 2: TIME SERIES WITH CLUSTERS =====
    st.write("### ‚è∞ Ph√¢n b·ªë c·ª•m theo th·ªùi gian (Cluster Distribution Over Time)")
    fig, ax = plt.subplots(figsize=(14, 6))
    colors = plt.cm.Set2(np.linspace(0, 1, k))
    for cluster in range(k):
        cluster_data = df[df["Cluster"] == cluster]
        ax.scatter(cluster_data["Date"], cluster_data["Close/Last"],
                  label=f"Cluster {cluster}", alpha=0.6, s=40, color=colors[cluster])
    ax.set_xlabel("Date", fontsize=11)
    ax.set_ylabel("Close Price ($)", fontsize=11)
    ax.set_title(f"Gold Price with K-Means Clusters (K={k})")
    ax.legend(loc='best')
    plt.xticks(rotation=45)
    st.pyplot(fig)
    
    # ===== CLUSTER STATISTICS =====
    st.write("### üìà Th·ªëng k√™ chi ti·∫øt t·ª´ng c·ª•m (Cluster Statistics)")
    cluster_stats = df.groupby("Cluster")[quantitative_cols].agg(['mean', 'min', 'max', 'std'])
    st.dataframe(cluster_stats, use_container_width=True)
    
    # ===== CLUSTER SIZES =====
    st.write("### üìä K√≠ch th∆∞·ªõc c·ª•m (Cluster Sizes)")
    cluster_sizes = df["Cluster"].value_counts().sort_index()
    
    col1, col2 = st.columns(2)
    with col1:
        fig, ax = plt.subplots(figsize=(8, 5))
        ax.bar(cluster_sizes.index, cluster_sizes.values, color=colors, edgecolor='black', linewidth=1.5)
        ax.set_xlabel("Cluster", fontsize=11)
        ax.set_ylabel("Number of Data Points", fontsize=11)
        ax.set_title(f"Cluster Size Distribution (K={k})")
        ax.set_xticks(range(k))
        for i, v in enumerate(cluster_sizes.values):
            ax.text(i, v + 5, str(v), ha='center', fontweight='bold')
        st.pyplot(fig)
    
    with col2:
        fig, ax = plt.subplots(figsize=(8, 5))
        ax.pie(cluster_sizes.values, labels=[f"Cluster {i}\n({v} points)" for i, v in enumerate(cluster_sizes.values)],
               colors=colors, autopct='%1.1f%%', startangle=90, explode=[0.05]*k)
        ax.set_title(f"Cluster Distribution Percentage (K={k})")
        st.pyplot(fig)
    
    # ===== CLUSTER CHARACTERISTICS =====
    st.write("### üîç ƒê·∫∑c ƒëi·ªÉm c·ªßa t·ª´ng c·ª•m (Cluster Characteristics)")
    for cluster in range(k):
        cluster_data = df[df["Cluster"] == cluster]
        st.write(f"**Cluster {cluster}:** {len(cluster_data)} ƒëi·ªÉm d·ªØ li·ªáu")
        
        char_text = f"- **Gi√° Close trung b√¨nh:** ${cluster_data['Close/Last'].mean():.2f} (Range: ${cluster_data['Close/Last'].min():.2f} - ${cluster_data['Close/Last'].max():.2f})\n"
        char_text += f"- **Kh·ªëi l∆∞·ª£ng trung b√¨nh:** {cluster_data['Volume'].mean():.0f}\n"
        char_text += f"- **Giai ƒëo·∫°n th·ªùi gian:** {cluster_data['Date'].min().date()} ‚Üí {cluster_data['Date'].max().date()}"
        st.write(char_text)
    
    # ===== SAMPLE DATA WITH CLUSTERS =====
    st.write("### üìù D·ªØ li·ªáu m·∫´u sau ph√¢n c·ª•m (Sample Data with Clusters)")
    display_cols = ["Date", "Open", "High", "Low", "Close/Last", "Volume", "Cluster"]
    st.dataframe(df[display_cols].head(20), use_container_width=True)
    
    # ===== MODEL EVALUATION =====
    st.write("### üìä ƒê√°nh gi√° m√¥ h√¨nh K-Means (Model Evaluation)")
    
    # Calculate inertia and silhouette score
    inertia = kmeans.inertia_
    silhouette = silhouette_score(df[quantitative_cols], df["Cluster"])
    davies_bouldin = davies_bouldin_score(df[quantitative_cols], df["Cluster"])
    calinski = calinski_harabasz_score(df[quantitative_cols], df["Cluster"])
    
    eval_metrics = {
        "Metric": ["Inertia", "Silhouette Score", "Davies-Bouldin Index", "Calinski-Harabasz Index"],
        "Value": [
            f"{inertia:.2f}",
            f"{silhouette:.4f}",
            f"{davies_bouldin:.4f}",
            f"{calinski:.2f}"
        ],
        "Interpretation": [
            "Sum of squared distances (Lower is better)",
            "Clustering quality (-1 to 1, Higher is better)",
            "Cluster separation (Lower is better)",
            "Cluster density (Higher is better)"
        ]
    }
    st.dataframe(pd.DataFrame(eval_metrics), use_container_width=True)
    
    st.write(f"""
    **Nh·∫≠n x√©t:**
    - **Silhouette Score = {silhouette:.4f}**: {'T·ªët ‚úì' if silhouette > 0.5 else 'Trung b√¨nh ‚ö†' if silhouette > 0.3 else 'C·∫ßn c·∫£i thi·ªán ‚úó'}
    - M√¥ h√¨nh ph√¢n c·ª•m gi√∫p hi·ªÉu r√µ c·∫•u tr√∫c d·ªØ li·ªáu gi√° v√†ng
    - C√°c c·ª•m c√≥ th·ªÉ ƒë·∫°i di·ªán cho c√°c giai ƒëo·∫°n ho·∫∑c xu h∆∞·ªõng gi√° kh√°c nhau
    """)
    
    # ===========================
    # LINEAR REGRESSION - FORECAST (N√ÇNG C·∫§P ‚úÖ)
    # ===========================
    st.markdown("---")
    st.subheader("üìà D·ª± ƒëo√°n gi√° v√†ng (Linear Regression + Regularization)")
    
    # 1) Ch·ªçn horizon + t·ª∑ l·ªá test
    col1, col2, col3 = st.columns(3)
    with col1:
        horizon = st.selectbox("Ch·ªçn b∆∞·ªõc d·ª± ƒëo√°n (t+h, theo NG√ÄY)", [1, 7, 14, 30, 60, 90, 180, 252], index=3)
    with col2:
        test_ratio = st.slider("T·ª∑ l·ªá test (time split)", 0.15, 0.35, 0.25, 0.05)
    with col3:
        reg_type = st.radio("Regularization:", ["Ridge (L2)", "Lasso (L1)", "ElasticNet"], index=0)
    
    # 2) T·∫°o dataset supervised
    lr_data, feat_cols = make_lr_dataset(df, horizon=horizon)
    
    # ===== L·ªåC FEATURES NHI·ªÑU =====
    feat_corr = lr_data[feat_cols + ["y"]].corr()["y"].drop("y")
    strong_features = feat_corr[feat_corr.abs() > 0.01].index.tolist()
    strong_features = [f for f in strong_features if f in feat_cols]
    
    if len(strong_features) < 3:
        st.warning("Qu√° √≠t feature sau l·ªçc, d√πng t·∫•t c·∫£ features")
        feat_cols_selected = feat_cols
    else:
        feat_cols_selected = strong_features
    
    st.write(f"**Features sau l·ªçc:** {len(feat_cols_selected)}/{len(feat_cols)} (lo·∫°i b·ªè noise)")
    
    # Time split
    train_df, test_df = time_split(lr_data, test_ratio=test_ratio)
    X_train, y_train = train_df[feat_cols_selected], train_df["y"]
    X_test, y_test = test_df[feat_cols_selected], test_df["y"]
    
    # ===== CHU·∫®N H√ìA D·ªÆ LI·ªÜU =====
    scaler_robust = RobustScaler()
    X_train_scaled = scaler_robust.fit_transform(X_train)
    X_test_scaled = scaler_robust.transform(X_test)
    
    # ===== REGULARIZATION + TRAINING =====
    alpha = st.slider("ƒê·ªô m·∫°nh regularization (Œ±)", 0.001, 1.0, 0.1, 0.01)
    
    if reg_type == "Ridge (L2)":
        lr = Ridge(alpha=alpha)
    elif reg_type == "Lasso (L1)":
        lr = Lasso(alpha=alpha, max_iter=10000)
    else:
        lr = ElasticNet(alpha=alpha, l1_ratio=0.5, max_iter=10000)
    
    lr.fit(X_train_scaled, y_train)
    y_pred_train = lr.predict(X_train_scaled)
    y_pred = lr.predict(X_test_scaled)
    
    # ===== METRICS =====
    mae = mean_absolute_error(y_test, y_pred)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    r2 = lr.score(X_test_scaled, y_test)
    
    c1, c2, c3, c4, c5 = st.columns(5)
    c1.metric("MAE", f"{mae:.4f}")
    c2.metric("RMSE", f"{rmse:.4f}")
    c3.metric("R¬≤ Score", f"{r2:.4f}")
    c4.metric("Test size", f"{len(test_df)}")
    c5.metric("Horizon", f"t+{horizon}d")
    
    # ===== ACTUAL vs PREDICTED =====
    st.write("### üìä Actual vs Predicted Return (tr√™n t·∫≠p test)")
    
    fig, ax = plt.subplots(figsize=(13, 5))
    ax.plot(test_df["Date"].values, y_test.values, label="Actual Return", linewidth=2, color='blue')
    ax.plot(test_df["Date"].values, y_pred, label="Predicted Return", linewidth=2, color='red', alpha=0.7)
    ax.fill_between(test_df["Date"].values, y_test.values, y_pred, alpha=0.2, color='gray')
    ax.set_title(f"Actual vs Predicted Return (t+{horizon} ng√†y)")
    ax.set_xlabel("Date")
    ax.set_ylabel("Return (% change)")
    ax.legend()
    plt.xticks(rotation=45)
    st.pyplot(fig)
    
    # ===== RESIDUAL ANALYSIS =====
    st.write("### üîç Ph√¢n t√≠ch sai s·ªë (Residuals)")
    
    residuals = y_test - y_pred
    
    fig, axes = plt.subplots(1, 2, figsize=(13, 4))
    
    axes[0].hist(residuals, bins=30, color='skyblue', edgecolor='black')
    axes[0].set_title("Distribution of Residuals")
    axes[0].set_xlabel("Residual (Actual - Predicted)")
    axes[0].axvline(0, color='red', linestyle='--', linewidth=2)
    
    axes[1].scatter(y_pred, residuals, alpha=0.6, s=30)
    axes[1].axhline(0, color='red', linestyle='--', linewidth=2)
    axes[1].set_title("Residual Plot")
    axes[1].set_xlabel("Predicted Return")
    axes[1].set_ylabel("Residual")
    
    st.pyplot(fig)
    
    residual_mean = residuals.mean()
    residual_std = residuals.std()
    st.write(f"**Residual Mean:** {residual_mean:.6f} (n√™n ‚âà 0)")
    st.write(f"**Residual Std:** {residual_std:.6f}")
    
    # ===== FEATURE COEFFICIENTS =====
    st.write("### üß† ƒê√≥ng g√≥p c·ªßa feature (Top 15 Coefficients)")
    coef_df = pd.DataFrame({"Feature": feat_cols_selected, "Coef": lr.coef_})
    coef_df["AbsCoef"] = coef_df["Coef"].abs()
    coef_df = coef_df.sort_values("AbsCoef", ascending=False)
    
    st.dataframe(coef_df.drop(columns=["AbsCoef"]).head(15), use_container_width=True)
    
    # ===========================
    # FORECAST T∆Ø∆†NG LAI (N√ÇNG C·∫§P) ‚úÖ‚úÖ‚úÖ
    # ===========================
    st.markdown("---")
    st.subheader("üîÆ D·ª± b√°o t∆∞∆°ng lai (252 ng√†y, v·ªõi kho·∫£ng tin c·∫≠y)")
    
    do_forecast = st.checkbox("B·∫≠t d·ª± b√°o t∆∞∆°ng lai (multi-step forecast)", value=True)
    
    if do_forecast:
        col1, col2, col3 = st.columns(3)
        with col1:
            n_steps = st.slider("S·ªë ng√†y d·ª± b√°o", 30, 252, 90, 5)
        with col2:
            confidence_level = st.select_slider("Kho·∫£ng tin c·∫≠y", [0.68, 0.85, 0.95], value=0.95)
        with col3:
            use_business_days = st.checkbox("D√πng Business days", value=True)
        
        # ===== H√ÄM CH√çNH: T·∫†YO FEATURES T·ª™ L·ªäCH S·ª¨ =====
        def build_features_from_history_scaled(hist_df: pd.DataFrame, scaler_obj, feat_cols_list) -> np.ndarray:
            """X√¢y d·ª±ng 1 d√≤ng feature t·ª´ l·ªãch s·ª≠, tr·∫£ v·ªÅ array ƒë√£ chu·∫©n h√≥a"""
            row = {}
            
            # lags
            for lag in [1, 2, 3, 5, 7, 14, 30]:
                if len(hist_df) >= lag:
                    row[f"close_lag_{lag}"] = hist_df["Close/Last"].iloc[-lag]
                else:
                    row[f"close_lag_{lag}"] = np.nan
            
            # rolling
            row["ma_7"] = hist_df["Close/Last"].rolling(7).mean().iloc[-1] if len(hist_df) >= 7 else np.nan
            row["ma_14"] = hist_df["Close/Last"].rolling(14).mean().iloc[-1] if len(hist_df) >= 14 else np.nan
            row["ma_20"] = hist_df["Close/Last"].rolling(20).mean().iloc[-1] if len(hist_df) >= 20 else np.nan
            row["ma_30"] = hist_df["Close/Last"].rolling(30).mean().iloc[-1] if len(hist_df) >= 30 else np.nan
            row["ma_60"] = hist_df["Close/Last"].rolling(60).mean().iloc[-1] if len(hist_df) >= 60 else np.nan
            row["ma_200"] = hist_df["Close/Last"].rolling(200).mean().iloc[-1] if len(hist_df) >= 200 else np.nan
            
            row["std_7"] = hist_df["Close/Last"].rolling(7).std().iloc[-1] if len(hist_df) >= 7 else np.nan
            row["std_14"] = hist_df["Close/Last"].rolling(14).std().iloc[-1] if len(hist_df) >= 14 else np.nan
            row["std_20"] = hist_df["Close/Last"].rolling(20).std().iloc[-1] if len(hist_df) >= 20 else np.nan
            
            # momentum
            row["momentum_7"] = hist_df["Close/Last"].iloc[-1] - hist_df["Close/Last"].iloc[-8] if len(hist_df) >= 8 else np.nan
            row["momentum_14"] = hist_df["Close/Last"].iloc[-1] - hist_df["Close/Last"].iloc[-15] if len(hist_df) >= 15 else np.nan
            row["momentum_30"] = hist_df["Close/Last"].iloc[-1] - hist_df["Close/Last"].iloc[-31] if len(hist_df) >= 31 else np.nan
            
            # ROC
            row["roc_7"] = (hist_df["Close/Last"].iloc[-1] - hist_df["Close/Last"].iloc[-8]) / hist_df["Close/Last"].iloc[-8] if len(hist_df) >= 8 else np.nan
            row["roc_14"] = (hist_df["Close/Last"].iloc[-1] - hist_df["Close/Last"].iloc[-15]) / hist_df["Close/Last"].iloc[-15] if len(hist_df) >= 15 else np.nan
            row["roc_30"] = (hist_df["Close/Last"].iloc[-1] - hist_df["Close/Last"].iloc[-31]) / hist_df["Close/Last"].iloc[-31] if len(hist_df) >= 31 else np.nan
            
            # RSI
            delta = hist_df["Close/Last"].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs))
            row["rsi_14"] = rsi.iloc[-1] if len(rsi) > 0 else np.nan
            
            # Bollinger Bands
            ma_20 = hist_df["Close/Last"].rolling(20).mean().iloc[-1] if len(hist_df) >= 20 else np.nan
            std_20 = hist_df["Close/Last"].rolling(20).std().iloc[-1] if len(hist_df) >= 20 else np.nan
            if pd.notna(ma_20) and pd.notna(std_20):
                bb_upper = ma_20 + 2 * std_20
                bb_lower = ma_20 - 2 * std_20
                row["bb_position"] = (hist_df["Close/Last"].iloc[-1] - bb_lower) / (bb_upper - bb_lower)
            else:
                row["bb_position"] = np.nan
            row["bb_upper"] = bb_upper if pd.notna(bb_upper) else np.nan
            row["bb_lower"] = bb_lower if pd.notna(bb_lower) else np.nan
            
            # Volume
            if "Volume" in hist_df.columns:
                row["vol_lag_1"] = hist_df["Volume"].iloc[-1]
                row["vol_ma_7"] = hist_df["Volume"].rolling(7).mean().iloc[-1] if len(hist_df) >= 7 else np.nan
                row["vol_ma_30"] = hist_df["Volume"].rolling(30).mean().iloc[-1] if len(hist_df) >= 30 else np.nan
                row["vol_std"] = hist_df["Volume"].rolling(7).std().iloc[-1] if len(hist_df) >= 7 else np.nan
                row["price_vol_trend"] = (hist_df["Close/Last"].iloc[-1] - hist_df["Close/Last"].iloc[-2]) / hist_df["Close/Last"].iloc[-2] * hist_df["Volume"].iloc[-1] if len(hist_df) >= 2 else np.nan
            
            # Chuy·ªÉn th√†nh DataFrame, ch·ªçn c·ªôt c·∫ßn thi·∫øt, chu·∫©n h√≥a
            df_row = pd.DataFrame([row])
            df_row = df_row[feat_cols_list]
            
            x_scaled = scaler_obj.transform(df_row)
            return x_scaled.flatten()
        
        # ===== RECURSIVE FORECAST =====
        hist = df[["Date", "Close/Last"] + (["Volume"] if "Volume" in df.columns else [])].copy()
        hist = hist.sort_values("Date").reset_index(drop=True)
        
        last_date = hist["Date"].iloc[-1]
        last_close = hist["Close/Last"].iloc[-1]
        
        if use_business_days:
            future_dates = pd.bdate_range(last_date + pd.Timedelta(days=1), periods=n_steps)
        else:
            future_dates = pd.date_range(last_date + pd.Timedelta(days=1), periods=n_steps, freq="D")
        
        preds = []
        preds_std = []
        temp_hist = hist.copy()
        
        sigma = residual_std
        
        for i, dt in enumerate(future_dates):
            try:
                x_next = build_features_from_history_scaled(temp_hist, scaler_robust, feat_cols_selected)
                
                if np.isnan(x_next).any():
                    preds.append(np.nan)
                    preds_std.append(np.nan)
                else:
                    y_next = lr.predict(x_next.reshape(1, -1))[0]
                    preds.append(y_next)
                    preds_std.append(sigma)
                    
                    # C·∫≠p nh·∫≠t l·ªãch s·ª≠
                    new_close = temp_hist["Close/Last"].iloc[-1] * (1 + y_next)
                    new_row = {"Date": pd.Timestamp(dt), "Close/Last": new_close}
                    if "Volume" in temp_hist.columns:
                        new_row["Volume"] = temp_hist["Volume"].iloc[-1]
                    temp_hist = pd.concat([temp_hist, pd.DataFrame([new_row])], ignore_index=True)
            except Exception as e:
                st.warning(f"L·ªói t·∫°i b∆∞·ªõc {i}: {e}")
                preds.append(np.nan)
                preds_std.append(np.nan)
        
        forecast_df = pd.DataFrame({
            "Date": future_dates,
            "Forecast_Return": preds,
            "Forecast_Std": preds_std
        })
        forecast_df = forecast_df.dropna().reset_index(drop=True)
        
        # T√≠nh Confidence Interval
        z_score = stats.norm.ppf((1 + confidence_level) / 2)
        forecast_df["CI_Lower"] = forecast_df["Forecast_Return"] - z_score * forecast_df["Forecast_Std"]
        forecast_df["CI_Upper"] = forecast_df["Forecast_Return"] + z_score * forecast_df["Forecast_Std"]
        
        # T√≠nh gi√° d·ª± b√°o
        forecast_df["Forecast_Price"] = last_close
        for idx in range(len(forecast_df)):
            forecast_df.loc[idx, "Forecast_Price"] = forecast_df.loc[idx-1, "Forecast_Price"] * (1 + forecast_df.loc[idx, "Forecast_Return"]) if idx > 0 else last_close * (1 + forecast_df.loc[idx, "Forecast_Return"])
        
        st.write("### üìä K·∫øt qu·∫£ d·ª± b√°o (Forecast Results)")
        st.dataframe(forecast_df, use_container_width=True)
        
        # Visualization
        fig, ax = plt.subplots(figsize=(14, 6))
        ax.plot(hist["Date"], hist["Close/Last"], label="Historical Price", linewidth=2, color='blue')
        ax.plot(forecast_df["Date"], forecast_df["Forecast_Price"], label="Forecast Price", linewidth=2, color='red', linestyle='--')
        ax.fill_between(forecast_df["Date"], forecast_df["Forecast_Price"] * (1 + forecast_df["CI_Lower"]), 
                        forecast_df["Forecast_Price"] * (1 + forecast_df["CI_Upper"]), 
                        alpha=0.2, color='red', label=f'{int(confidence_level*100)}% Confidence Interval')
        ax.set_xlabel("Date")
        ax.set_ylabel("Price (USD/oz)")
        ax.set_title(f"Gold Price Forecast ({n_steps} days)")
        ax.legend()
        plt.xticks(rotation=45)
        st.pyplot(fig)