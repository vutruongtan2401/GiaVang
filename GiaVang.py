# ==========================================================
# DATA MINING PROJECT - GOLD PRICE DATA
# Dataset: goldstock v2.csv
# B1 ‚Üí B5 (EDA-focused, model minh h·ªça, c√≥ GUI)
# ==========================================================

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import streamlit as st

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

plt.style.use('ggplot')

# ==========================================================
# LOAD DATA - B0: DATASET OVERVIEW
# ==========================================================
# Load CSV with semicolon delimiter
original_df = pd.read_csv("goldstock v2.csv", sep=";")

print("Columns in CSV:", original_df.columns.tolist())
print("First few rows:")
print(original_df.head())

# X√≥a c·ªôt index kh√¥ng c·∫ßn thi·∫øt n·∫øu c√≥
if "Column1" in original_df.columns:
    original_df.drop(columns=["Column1"], inplace=True)

if "Unnamed: 0" in original_df.columns:
    original_df.drop(columns=["Unnamed: 0"], inplace=True)

# ƒê·∫£m b·∫£o c√°c c·ªôt ch·ª©a d·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω ƒë√∫ng
# X·ª≠ l√Ω kho·∫£ng tr·∫Øng d∆∞ th·ª´a
original_df.columns = original_df.columns.str.strip()

# Chuy·ªÉn ƒë·ªïi c·ªôt s·ªë sang numeric type
numeric_cols = ["Volume", "Open", "High", "Low", "Close/Last"]
for col in numeric_cols:
    if col in original_df.columns:
        original_df[col] = pd.to_numeric(original_df[col], errors='coerce')

# Chuy·ªÉn Date sang datetime v·ªõi x·ª≠ l√Ω l·ªói
try:
    # Th·ª≠ nhi·ªÅu ƒë·ªãnh d·∫°ng kh√°c nhau (DD/MM/YYYY l√† ph·ªï bi·∫øn)
    original_df["Date"] = pd.to_datetime(original_df["Date"], format="%d/%m/%Y", errors='coerce')
    
    # Ki·ªÉm tra v√† lo·∫°i b·ªè c√°c gi√° tr·ªã null sau khi convert
    null_dates = original_df["Date"].isnull().sum()
    if null_dates > 0:
        print(f"Warning: {null_dates} rows with invalid dates will be removed")
        original_df = original_df.dropna(subset=["Date"])
        
except Exception as e:
    print(f"Error converting Date column: {e}")
    print("Attempting alternative date parsing...")
    original_df["Date"] = pd.to_datetime(original_df["Date"], infer_datetime_format=True, errors='coerce')
    original_df = original_df.dropna(subset=["Date"])

# S·∫Øp x·∫øp theo th·ªùi gian (t·ª´ c≈© ƒë·∫øn m·ªõi)
original_df.sort_values(by="Date", inplace=True, ascending=True)
original_df.reset_index(drop=True, inplace=True)

print(f"Data loaded successfully: {len(original_df)} rows")
print("Columns after processing:", original_df.columns.tolist())
print(original_df.head())

# ==========================================================
# B2 ‚Äì DATA CLEANING (TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU)
# ==========================================================
# Clone m·ªôt b·∫£n sao ƒë·ªÉ gi·ªØ nguy√™n d·ªØ li·ªáu g·ªëc
df = original_df.copy()

# Ki·ªÉm tra d·ªØ li·ªáu thi·∫øu
missing_data = df.isnull().sum() * 100 / df.shape[0]

# X√≥a duplicate
df = df[df.duplicated() == False].reset_index(drop=True)

# X√≥a c√°c h√†ng c√≥ gi√° tr·ªã NaN sau khi x·ª≠ l√Ω
df = df.dropna(subset=["Date", "Open", "High", "Low", "Close/Last", "Volume"])

# Ki·ªÉm tra logic gi√° (High >= Open, Close/Last, Low; Low <= Open, Close/Last)
if "High" in df.columns and "Low" in df.columns:
    df = df[
        (df["High"] >= df["Open"]) &
        (df["High"] >= df["Close/Last"]) &
        (df["High"] >= df["Low"]) &
        (df["Low"] <= df["Open"]) &
        (df["Low"] <= df["Close/Last"])
    ]

df.reset_index(drop=True, inplace=True)

# ==========================================================
# B1 ‚Äì M√î T·∫¢ D·ªÆ LI·ªÜU (DATA OVERVIEW)
# ==========================================================
quantitative_cols = df.select_dtypes(include=["int64", "float64"]).columns.tolist()
qualitative_cols = df.select_dtypes(exclude=["int64", "float64"]).columns.tolist()

# ==========================================================
# STREAMLIT GUI CONFIGURATION
# ==========================================================
st.set_page_config(page_title="Gold Price Data Mining", layout="wide")
st.title("üìä Gold Price Data Mining Project")
st.markdown("---")

# Create tabs for each phase
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "B1 - Data Overview",
    "B2 - Data Cleaning",
    "B3 - Exploratory Analysis",
    "B4 - Correlation & Dimensionality",
    "B5 - Model & Visualization"
])

# ==========================================================
# TAB 1: B1 ‚Äì DATA OVERVIEW (M√î T·∫¢ D·ªÆ LI·ªÜU)
# ==========================================================
with tab1:
    st.header("B1 ‚Äì M√¥ t·∫£ d·ªØ li·ªáu (Dataset Overview)")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total Rows", df.shape[0])
    with col2:
        st.metric("Total Columns", df.shape[1])
    with col3:
        st.metric("Date Range", f"{df['Date'].min().date()} to {df['Date'].max().date()}")
    
    st.write("### üìã Danh s√°ch c√°c c·ªôt (Dataset Columns)")
    st.write(df.columns.tolist())
    
    st.write("### üìä Ph√¢n lo·∫°i d·ªØ li·ªáu (Data Types Classification)")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**ƒê·ªãnh l∆∞·ª£ng (Quantitative):**")
        st.write(f"- S·ªë c·ªôt: **{len(quantitative_cols)}**")
        for col in quantitative_cols:
            st.write(f"  - `{col}` ({df[col].dtype})")
    
    with col2:
        st.write("**ƒê·ªãnh t√≠nh (Qualitative):**")
        st.write(f"- S·ªë c·ªôt: **{len(qualitative_cols)}**")
        for col in qualitative_cols:
            st.write(f"  - `{col}` ({df[col].dtype})")
    
    st.write("### üìà Th·ªëng k√™ m√¥ t·∫£ chi ti·∫øt (Descriptive Statistics)")
    st.dataframe(df[quantitative_cols].describe(), use_container_width=True)
    
    st.write("### üîç Th√¥ng tin chi ti·∫øt c√°c c·ªôt (Detailed Column Info)")
    info_data = {
        "Column": df.columns,
        "Data Type": df.dtypes.astype(str),
        "Non-Null Count": df.count(),
        "Null Count": df.isnull().sum(),
        "Min": [df[col].min() if col in quantitative_cols else "N/A" for col in df.columns],
        "Max": [df[col].max() if col in quantitative_cols else "N/A" for col in df.columns],
    }
    st.dataframe(pd.DataFrame(info_data), use_container_width=True)
    
    st.write("### üìù D·ªØ li·ªáu m·∫´u (Sample Data)")
    st.dataframe(df.head(10), use_container_width=True)

# ==========================================================
# TAB 2: B2 ‚Äì DATA CLEANING (TI·ªÄN X·ª¨ L√ù)
# ==========================================================
with tab2:
    st.header("B2 ‚Äì Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu (Data Cleaning)")
    
    st.write("### üîç 1. Ki·ªÉm tra d·ªØ li·ªáu thi·∫øu (Missing Data)")
    missing_count = original_df.isnull().sum()
    missing_percent = (missing_count / len(original_df)) * 100
    missing_df = pd.DataFrame({
        "Column": missing_count.index,
        "Missing Count": missing_count.values,
        "Missing %": missing_percent.values
    })
    st.dataframe(missing_df[missing_df["Missing Count"] > 0] if missing_df["Missing Count"].sum() > 0 
                 else pd.DataFrame({"Status": ["‚úÖ No missing data found"]}), use_container_width=True)
    
    st.write("### üîÑ 2. Ki·ªÉm tra d·ªØ li·ªáu tr√πng l·∫∑p (Duplicate Data)")
    col1, col2 = st.columns(2)
    with col1:
        st.write(f"‚ùå **Tr∆∞·ªõc x·ª≠ l√Ω:** {original_df.duplicated().sum()} d√≤ng tr√πng l·∫∑p")
    with col2:
        st.write(f"‚úÖ **Sau x·ª≠ l√Ω:** {df.duplicated().sum()} d√≤ng tr√πng l·∫∑p")
    
    st.write("### üö® 3. Ph√°t hi·ªán Noise & Outliers")
    
    # Ki·ªÉm tra outliers b·∫±ng IQR
    st.write("**Ph∆∞∆°ng ph√°p IQR (Interquartile Range):**")
    outlier_summary = []
    
    for col in quantitative_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outlier_count = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
        outlier_summary.append({
            "Column": col,
            "Q1": round(Q1, 2),
            "Q3": round(Q3, 2),
            "IQR": round(IQR, 2),
            "Lower Bound": round(lower_bound, 2),
            "Upper Bound": round(upper_bound, 2),
            "Outlier Count": outlier_count
        })
    
    outlier_df = pd.DataFrame(outlier_summary)
    st.dataframe(outlier_df, use_container_width=True)
    
    # Visualize outliers
    fig, axes = plt.subplots(len(quantitative_cols), 1, figsize=(10, 3*len(quantitative_cols)))
    if len(quantitative_cols) == 1:
        axes = [axes]
    
    for idx, col in enumerate(quantitative_cols):
        sns.boxplot(data=df, y=col, ax=axes[idx], color='steelblue')
        axes[idx].set_title(f"Outlier Detection: {col}")
    
    plt.tight_layout()
    st.pyplot(fig)
    
    st.write("### ‚úÖ 4. C√°c quy t·∫Øc x√°c th·ª±c d·ªØ li·ªáu (Data Validation Rules)")
    st.write("""
    ‚úîÔ∏è **High >= Open, Close/Last, Low** - Gi√° cao nh·∫•t >= t·∫•t c·∫£ c√°c m·ª©c gi√° kh√°c
    ‚úîÔ∏è **Low <= Open, Close/Last** - Gi√° th·∫•p nh·∫•t <= t·∫•t c·∫£ c√°c m·ª©c gi√° kh√°c
    ‚úîÔ∏è **Volume >= 0** - Kh·ªëi l∆∞·ª£ng kh√¥ng √¢m
    ‚úîÔ∏è **Date sorted in ascending order** - D·ªØ li·ªáu s·∫Øp x·∫øp theo th·ªùi gian
    ‚úîÔ∏è **Duplicates removed** - Lo·∫°i b·ªè d√≤ng tr√πng l·∫∑p
    """)
    
    st.write("### üìä 5. So s√°nh d·ªØ li·ªáu tr∆∞·ªõc & sau l√†m s·∫°ch")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Rows (Before)", original_df.shape[0])
    with col2:
        st.metric("Rows (After)", df.shape[0])
    with col3:
        st.metric("Rows Removed", original_df.shape[0] - df.shape[0])
    
    st.write("### üìù D·ªØ li·ªáu m·∫´u sau l√†m s·∫°ch (Sample Cleaned Data)")
    st.dataframe(df.head(10), use_container_width=True)

# ==========================================================
# TAB 3: B3 ‚Äì EXPLORATORY DATA ANALYSIS (KHAI PH√Å D·ªÆ LI·ªÜU)
# ==========================================================
with tab3:
    st.header("B3 ‚Äì Khai ph√° d·ªØ li·ªáu (Exploratory Data Analysis)")
    
    # ===== UNIVARIATE ANALYSIS =====
    st.subheader("üìä Ph√¢n t√≠ch ƒë∆°n bi·∫øn (Univariate Analysis)")
    
    col_univariate = st.selectbox(
        "Ch·ªçn c·ªôt ƒë·ªãnh l∆∞·ª£ng ƒë·ªÉ ph√¢n t√≠ch",
        quantitative_cols,
        key="univariate"
    )
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Histogram with KDE
        fig, ax = plt.subplots(figsize=(10, 5))
        sns.histplot(df[col_univariate], kde=True, ax=ax, color='steelblue', bins=30)
        ax.set_title(f"Distribution of {col_univariate}")
        ax.set_xlabel(col_univariate)
        ax.set_ylabel("Frequency")
        st.pyplot(fig)
    
    with col2:
        # Box plot
        fig, ax = plt.subplots(figsize=(10, 5))
        sns.boxplot(data=df, y=col_univariate, ax=ax, color='lightcoral')
        ax.set_title(f"Box Plot of {col_univariate}")
        st.pyplot(fig)
    
    # Statistical summary
    st.write(f"### üìà Th·ªëng k√™ m√¥ t·∫£: {col_univariate}")
    stats_data = {
        "Metric": ["Count", "Mean", "Std Dev", "Min", "25%", "Median", "75%", "Max", "Range", "Skewness"],
        "Value": [
            df[col_univariate].count(),
            round(df[col_univariate].mean(), 2),
            round(df[col_univariate].std(), 2),
            round(df[col_univariate].min(), 2),
            round(df[col_univariate].quantile(0.25), 2),
            round(df[col_univariate].median(), 2),
            round(df[col_univariate].quantile(0.75), 2),
            round(df[col_univariate].max(), 2),
            round(df[col_univariate].max() - df[col_univariate].min(), 2),
            round(df[col_univariate].skew(), 2),
        ]
    }
    st.dataframe(pd.DataFrame(stats_data), use_container_width=True)
    
    # ===== BIVARIATE ANALYSIS =====
    st.subheader("üìà Ph√¢n t√≠ch ƒëa bi·∫øn (Bivariate Analysis)")
    
    col1, col2 = st.columns(2)
    with col1:
        x_col = st.selectbox("Ch·ªçn tr·ª•c X", quantitative_cols, index=0, key="x_axis")
    with col2:
        y_col = st.selectbox("Ch·ªçn tr·ª•c Y", quantitative_cols, index=1 if len(quantitative_cols) > 1 else 0, key="y_axis")
    
    fig, ax = plt.subplots(figsize=(10, 6))
    sns.scatterplot(x=df[x_col], y=df[y_col], ax=ax, alpha=0.6, color='steelblue', s=50)
    # Th√™m trendline
    z = np.polyfit(df[x_col], df[y_col], 1)
    p = np.poly1d(z)
    ax.plot(df[x_col].sort_values(), p(df[x_col].sort_values()), "r--", linewidth=2, label='Trend')
    ax.set_title(f"Scatter Plot: {x_col} vs {y_col}")
    ax.legend()
    st.pyplot(fig)
    
    # Correlation coefficient
    corr_coef = df[x_col].corr(df[y_col])
    st.write(f"**H·ªá s·ªë t∆∞∆°ng quan Pearson:** {corr_coef:.4f}")
    
    # ===== TIME SERIES ANALYSIS =====
    st.subheader("‚è∞ Ph√¢n t√≠ch chu·ªói th·ªùi gian (Time Series Analysis)")
    
    fig, ax = plt.subplots(figsize=(12, 5))
    ax.plot(df["Date"], df["Close/Last"], linewidth=2, color='steelblue', label='Close Price')
    ax.fill_between(df["Date"], df["Low"], df["High"], alpha=0.2, color='lightblue', label='High-Low Range')
    ax.set_xlabel("Date")
    ax.set_ylabel("Price (USD/oz)")
    ax.set_title("Xu h∆∞·ªõng gi√° v√†ng (Gold Price Trend)")
    ax.legend()
    plt.xticks(rotation=45)
    st.pyplot(fig)
    
    # Th·ªëng k√™ chu·ªói th·ªùi gian
    st.write("### üìä Th·ªëng k√™ chu·ªói th·ªùi gian:")
    time_stats = {
        "Metric": ["Avg Close Price", "Max Close Price", "Min Close Price", "Price Range", "Volatility (Std)"],
        "Value": [
            f"${df['Close/Last'].mean():.2f}",
            f"${df['Close/Last'].max():.2f}",
            f"${df['Close/Last'].min():.2f}",
            f"${df['Close/Last'].max() - df['Close/Last'].min():.2f}",
            f"${df['Close/Last'].std():.2f}"
        ]
    }
    st.dataframe(pd.DataFrame(time_stats), use_container_width=True)
    
    # ===== VOLUME ANALYSIS =====
    st.subheader("üìä Ph√¢n t√≠ch kh·ªëi l∆∞·ª£ng giao d·ªãch (Volume Analysis)")
    
    fig, ax = plt.subplots(figsize=(12, 5))
    ax.bar(df["Date"], df["Volume"], color='steelblue', alpha=0.7, width=0.8)
    ax.set_xlabel("Date")
    ax.set_ylabel("Volume")
    ax.set_title("Kh·ªëi l∆∞·ª£ng giao d·ªãch (Trading Volume Over Time)")
    plt.xticks(rotation=45)
    st.pyplot(fig)
    
    # Volume statistics
    st.write("### üìä Th·ªëng k√™ kh·ªëi l∆∞·ª£ng:")
    volume_stats = {
        "Metric": ["Avg Volume", "Max Volume", "Min Volume", "Total Volume"],
        "Value": [
            f"{df['Volume'].mean():.0f}",
            f"{df['Volume'].max():.0f}",
            f"{df['Volume'].min():.0f}",
            f"{df['Volume'].sum():.0f}"
        ]
    }
    st.dataframe(pd.DataFrame(volume_stats), use_container_width=True)

# ==========================================================
# TAB 4: B4 ‚Äì CORRELATION & DIMENSIONALITY REDUCTION
# ==========================================================
with tab4:
    st.header("B4 ‚Äì Ma tr·∫≠n t∆∞∆°ng quan & Gi·∫£m chi·ªÅu (Correlation & PCA)")
    
    # ===== CORRELATION MATRIX =====
    st.subheader("üîó Ma tr·∫≠n t∆∞∆°ng quan (Correlation Matrix)")
    
    corr_matrix = df[quantitative_cols].corr()
    
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", center=0, 
                square=True, ax=ax, fmt=".2f", cbar_kws={'label': 'Correlation'})
    ax.set_title("Correlation Matrix of Quantitative Variables")
    st.pyplot(fig)
    
    st.write("### üß† Ph√¢n t√≠ch & L·∫≠p lu·∫≠n Gi·ªØ/B·ªè c·ªôt (Analysis & Column Selection Reasoning)")
    
    # T√≠nh t∆∞∆°ng quan cao
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > 0.95:
                high_corr_pairs.append({
                    "Column 1": corr_matrix.columns[i],
                    "Column 2": corr_matrix.columns[j],
                    "Correlation": round(corr_matrix.iloc[i, j], 4)
                })
    
    if high_corr_pairs:
        st.write("**C√°c c·ªôt c√≥ t∆∞∆°ng quan r·∫•t cao (> 0.95):**")
        st.dataframe(pd.DataFrame(high_corr_pairs), use_container_width=True)
    
    st.write("""
    **K·∫øt lu·∫≠n & Quy·∫øt ƒë·ªãnh:**
    
    1. **Open, High, Low, Close/Last** c√≥ t∆∞∆°ng quan r·∫•t cao (> 0.95)
       - ‚ùå Gi·ªØ t·∫•t c·∫£ 4 c·ªôt l√† d∆∞ th·ª´a
       - ‚úÖ **Gi·ªØ:** `Close/Last` (gi√° ƒë√≥ng c·ª≠a - ch·ªâ b√°o ch√≠nh)
       - ‚ùå **B·ªè:** `Open`, `High`, `Low` (c√≥ th·ªÉ suy ra t·ª´ Close/Last)
    
    2. **Volume** t∆∞∆°ng quan y·∫øu v·ªõi c√°c c·ªôt gi√°
       - ‚úÖ **Gi·ªØ:** `Volume` (th√¥ng tin ƒë·ªôc l·∫≠p, h·ªØu √≠ch)
       - Kh·ªëi l∆∞·ª£ng giao d·ªãch ph·∫£n √°nh m·ª©c ƒë·ªô quan t√¢m c·ªßa th·ªã tr∆∞·ªùng
    
    **K·∫øt qu·∫£ cu·ªëi c√πng:**
    - ‚úÖ Gi·ªØ: `Close/Last`, `Volume`
    - ‚ùå B·ªè: `Open`, `High`, `Low` (d∆∞ th·ª´a, t∆∞∆°ng quan cao)
    """)
    
    # ===== DIMENSIONALITY REDUCTION WITH PCA =====
    st.subheader("üéØ Gi·∫£m chi·ªÅu d·ªØ li·ªáu (PCA - Principal Component Analysis)")
    
    # Prepare data for PCA
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df[quantitative_cols])
    
    # Apply PCA with all components
    pca_full = PCA()
    pca_full.fit(X_scaled)
    
    # Show cumulative variance explained
    cumsum_var = np.cumsum(pca_full.explained_variance_ratio_)
    
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.plot(range(1, len(cumsum_var)+1), cumsum_var, 'bo-', linewidth=2, markersize=8)
    ax.axhline(y=0.95, color='r', linestyle='--', label='95% Variance')
    ax.axhline(y=0.90, color='orange', linestyle='--', label='90% Variance')
    ax.set_xlabel("Number of Principal Components")
    ax.set_ylabel("Cumulative Explained Variance")
    ax.set_title("Cumulative Explained Variance by PCA Components")
    ax.legend()
    ax.grid(True, alpha=0.3)
    st.pyplot(fig)
    
    # Apply PCA with 2 components
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)
    
    # Visualization
    fig, ax = plt.subplots(figsize=(10, 6))
    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=range(len(df)), 
                        cmap='viridis', alpha=0.6, s=50)
    ax.set_xlabel(f"PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)")
    ax.set_ylabel(f"PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)")
    ax.set_title("PCA Projection of Gold Stock Data (2D)")
    plt.colorbar(scatter, ax=ax, label='Time Index')
    st.pyplot(fig)
    
    st.write("### üìä PCA Chi ti·∫øt (PCA Detailed Results)")
    pca_stats = {
        "Component": [f"PC{i+1}" for i in range(len(pca.explained_variance_ratio_))],
        "Explained Variance %": [f"{v:.2%}" for v in pca.explained_variance_ratio_],
        "Cumulative Variance %": [f"{c:.2%}" for c in np.cumsum(pca.explained_variance_ratio_)]
    }
    st.dataframe(pd.DataFrame(pca_stats), use_container_width=True)
    
    st.write(f"""
    **Nh·∫≠n x√©t:**
    - PC1 explains: **{pca.explained_variance_ratio_[0]:.2%}** c·ªßa ph∆∞∆°ng sai
    - PC2 explains: **{pca.explained_variance_ratio_[1]:.2%}** c·ªßa ph∆∞∆°ng sai
    - T·ªïng c·ªông: **{sum(pca.explained_variance_ratio_):.2%}** ph∆∞∆°ng sai ƒë∆∞·ª£c gi·∫£i th√≠ch
    
    **K·∫øt lu·∫≠n:** 2 th√†nh ph·∫ßn ch√≠nh gi·∫£i th√≠ch **{sum(pca.explained_variance_ratio_):.2%}** ph∆∞∆°ng sai,
    t·ª©c l√† gi·∫£m ƒë∆∞·ª£c t·ª´ {len(quantitative_cols)} chi·ªÅu xu·ªëng 2 chi·ªÅu m√† v·∫´n gi·ªØ l·∫°i h·∫ßu h·∫øt th√¥ng tin.
    """)
    
    # Feature loadings
    st.write("### üîç ƒê√≥ng g√≥p c·ªßa t·ª´ng bi·∫øn v√†o PC (Feature Loadings)")
    loadings_df = pd.DataFrame(
        pca.components_.T,
        columns=[f"PC{i+1}" for i in range(len(pca.components_))],
        index=quantitative_cols
    )
    st.dataframe(loadings_df, use_container_width=True)

# ==========================================================
# TAB 5: B5 ‚Äì MACHINE LEARNING MODEL & INTERACTIVE VISUALIZATION
# ==========================================================
with tab5:
    st.header("B5 ‚Äì M√¥ h√¨nh ML & Tr·ª±c quan h√≥a t∆∞∆°ng t√°c (Model & Visualization)")
    
    st.info("‚ö†Ô∏è **L∆∞u √Ω:** M√¥ h√¨nh K-Means d∆∞·ªõi ƒë√¢y ch·ªâ mang t√≠nh minh h·ªça ƒë·ªÉ ph√¢n c·ª•m d·ªØ li·ªáu. M·ª•c ƒë√≠ch l√† l√†m r√µ c·∫•u tr√∫c d·ªØ li·ªáu, kh√¥ng ƒë√°nh gi√° cao hi·ªáu su·∫•t d·ª± b√°o.")
    
    # ===== KMEANS CLUSTERING =====
    st.subheader("üéØ Ph√¢n c·ª•m d·ªØ li·ªáu (K-Means Clustering)")
    
    k = st.slider(
        "Ch·ªçn s·ªë c·ª•m (Select number of clusters)",
        min_value=2,
        max_value=6,
        value=3,
        step=1
    )
    
    # Apply KMeans
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    df["Cluster"] = kmeans.fit_predict(df[quantitative_cols])
    
    # ===== VISUALIZATION 1: SCATTER PLOTS =====
    st.write("### üìä Bi·ªÉu ƒë·ªì ph√¢n c·ª•m (Cluster Scatter Plots)")
    col1, col2 = st.columns(2)
    
    with col1:
        fig, ax = plt.subplots(figsize=(10, 6))
        scatter = ax.scatter(df["Open"], df["Close/Last"], 
                           c=df["Cluster"], cmap='Set2', s=80, alpha=0.6, edgecolors='black', linewidth=0.5)
        ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
                  c='red', marker='X', s=300, edgecolors='black', linewidth=2,
                  label='Centroids', zorder=5)
        ax.set_xlabel("Open Price ($)", fontsize=11)
        ax.set_ylabel("Close Price ($)", fontsize=11)
        ax.set_title(f"K-Means Clustering (Open vs Close) - K={k}")
        ax.legend()
        cbar = plt.colorbar(scatter, ax=ax, label='Cluster')
        st.pyplot(fig)
    
    with col2:
        fig, ax = plt.subplots(figsize=(10, 6))
        scatter = ax.scatter(df["Low"], df["High"],
                           c=df["Cluster"], cmap='Set2', s=80, alpha=0.6, edgecolors='black', linewidth=0.5)
        ax.scatter(kmeans.cluster_centers_[:, 2], kmeans.cluster_centers_[:, 3],
                  c='red', marker='X', s=300, edgecolors='black', linewidth=2,
                  label='Centroids', zorder=5)
        ax.set_xlabel("Low Price ($)", fontsize=11)
        ax.set_ylabel("High Price ($)", fontsize=11)
        ax.set_title(f"K-Means Clustering (Low vs High) - K={k}")
        ax.legend()
        cbar = plt.colorbar(scatter, ax=ax, label='Cluster')
        st.pyplot(fig)
    
    # ===== VISUALIZATION 2: TIME SERIES WITH CLUSTERS =====
    st.write("### ‚è∞ Ph√¢n b·ªë c·ª•m theo th·ªùi gian (Cluster Distribution Over Time)")
    fig, ax = plt.subplots(figsize=(14, 6))
    colors = plt.cm.Set2(np.linspace(0, 1, k))
    for cluster in range(k):
        cluster_data = df[df["Cluster"] == cluster]
        ax.scatter(cluster_data["Date"], cluster_data["Close/Last"],
                  label=f"Cluster {cluster}", alpha=0.6, s=40, color=colors[cluster])
    ax.set_xlabel("Date", fontsize=11)
    ax.set_ylabel("Close Price ($)", fontsize=11)
    ax.set_title(f"Gold Price with K-Means Clusters (K={k})")
    ax.legend(loc='best')
    plt.xticks(rotation=45)
    st.pyplot(fig)
    
    # ===== CLUSTER STATISTICS =====
    st.write("### üìà Th·ªëng k√™ chi ti·∫øt t·ª´ng c·ª•m (Cluster Statistics)")
    cluster_stats = df.groupby("Cluster")[quantitative_cols].agg(['mean', 'min', 'max', 'std'])
    st.dataframe(cluster_stats, use_container_width=True)
    
    # ===== CLUSTER SIZES =====
    st.write("### üìä K√≠ch th∆∞·ªõc c·ª•m (Cluster Sizes)")
    cluster_sizes = df["Cluster"].value_counts().sort_index()
    
    col1, col2 = st.columns(2)
    with col1:
        fig, ax = plt.subplots(figsize=(8, 5))
        ax.bar(cluster_sizes.index, cluster_sizes.values, color=colors, edgecolor='black', linewidth=1.5)
        ax.set_xlabel("Cluster", fontsize=11)
        ax.set_ylabel("Number of Data Points", fontsize=11)
        ax.set_title(f"Cluster Size Distribution (K={k})")
        ax.set_xticks(range(k))
        for i, v in enumerate(cluster_sizes.values):
            ax.text(i, v + 5, str(v), ha='center', fontweight='bold')
        st.pyplot(fig)
    
    with col2:
        fig, ax = plt.subplots(figsize=(8, 5))
        ax.pie(cluster_sizes.values, labels=[f"Cluster {i}\n({v} points)" for i, v in enumerate(cluster_sizes.values)],
               colors=colors, autopct='%1.1f%%', startangle=90, explode=[0.05]*k)
        ax.set_title(f"Cluster Distribution Percentage (K={k})")
        st.pyplot(fig)
    
    # ===== CLUSTER CHARACTERISTICS =====
    st.write("### üîç ƒê·∫∑c ƒëi·ªÉm c·ªßa t·ª´ng c·ª•m (Cluster Characteristics)")
    for cluster in range(k):
        cluster_data = df[df["Cluster"] == cluster]
        st.write(f"**Cluster {cluster}:** {len(cluster_data)} ƒëi·ªÉm d·ªØ li·ªáu")
        
        char_text = f"- **Gi√° Close trung b√¨nh:** ${cluster_data['Close/Last'].mean():.2f} (Range: ${cluster_data['Close/Last'].min():.2f} - ${cluster_data['Close/Last'].max():.2f})\n"
        char_text += f"- **Kh·ªëi l∆∞·ª£ng trung b√¨nh:** {cluster_data['Volume'].mean():.0f}\n"
        char_text += f"- **Giai ƒëo·∫°n th·ªùi gian:** {cluster_data['Date'].min().date()} ‚Üí {cluster_data['Date'].max().date()}"
        st.write(char_text)
    
    # ===== SAMPLE DATA WITH CLUSTERS =====
    st.write("### üìù D·ªØ li·ªáu m·∫´u sau ph√¢n c·ª•m (Sample Data with Clusters)")
    display_cols = ["Date", "Open", "High", "Low", "Close/Last", "Volume", "Cluster"]
    st.dataframe(df[display_cols].head(20), use_container_width=True)
    
    # ===== MODEL EVALUATION =====
    st.write("### üìä ƒê√°nh gi√° m√¥ h√¨nh K-Means (Model Evaluation)")
    
    # Calculate inertia and silhouette score
    from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
    
    inertia = kmeans.inertia_
    silhouette = silhouette_score(df[quantitative_cols], df["Cluster"])
    davies_bouldin = davies_bouldin_score(df[quantitative_cols], df["Cluster"])
    calinski = calinski_harabasz_score(df[quantitative_cols], df["Cluster"])
    
    eval_metrics = {
        "Metric": ["Inertia", "Silhouette Score", "Davies-Bouldin Index", "Calinski-Harabasz Index"],
        "Value": [
            f"{inertia:.2f}",
            f"{silhouette:.4f}",
            f"{davies_bouldin:.4f}",
            f"{calinski:.2f}"
        ],
        "Interpretation": [
            "Sum of squared distances (Lower is better)",
            "Clustering quality (-1 to 1, Higher is better)",
            "Cluster separation (Lower is better)",
            "Cluster density (Higher is better)"
        ]
    }
    st.dataframe(pd.DataFrame(eval_metrics), use_container_width=True)
    
    st.write(f"""
    **Nh·∫≠n x√©t:**
    - **Silhouette Score = {silhouette:.4f}**: {'T·ªët ‚úì' if silhouette > 0.5 else 'Trung b√¨nh ‚ö†' if silhouette > 0.3 else 'C·∫ßn c·∫£i thi·ªán ‚úó'}
    - M√¥ h√¨nh ph√¢n c·ª•m gi√∫p hi·ªÉu r√µ c·∫•u tr√∫c d·ªØ li·ªáu gi√° v√†ng
    - C√°c c·ª•m c√≥ th·ªÉ ƒë·∫°i di·ªán cho c√°c giai ƒëo·∫°n ho·∫∑c xu h∆∞·ªõng gi√° kh√°c nhau
    """)
    
    # ==========================================================
    # LINEAR REGRESSION - PRICE PREDICTION
    # ==========================================================
    st.markdown("---")
    st.subheader("üìà D·ª± ƒëo√°n gi√° v√†ng (Linear Regression Prediction)")
    
    st.info("üîÆ **M√¥ h√¨nh Linear Regression ƒë·ªÉ d·ª± ƒëo√°n gi√° v√†ng ƒë·∫øn nƒÉm 2027**")
    
    # Prepare data for Linear Regression
    # Convert Date to numeric (days since first date)
    df_model = df.copy()
    df_model['Days'] = (df_model['Date'] - df_model['Date'].min()).dt.days
    
    # Features and target
    X = df_model[['Days']].values
    y = df_model['Close/Last'].values
    
    # Train Linear Regression model
    lr_model = LinearRegression()
    lr_model.fit(X, y)
    
    # Predictions on training data
    y_pred_train = lr_model.predict(X)
    
    # Calculate metrics
    mse = mean_squared_error(y, y_pred_train)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y, y_pred_train)
    r2 = r2_score(y, y_pred_train)
    
    # Display model performance
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("R¬≤ Score", f"{r2:.4f}")
    with col2:
        st.metric("RMSE", f"${rmse:.2f}")
    with col3:
        st.metric("MAE", f"${mae:.2f}")
    with col4:
        st.metric("MSE", f"${mse:.2f}")
    
    # Create future dates up to 2027
    last_date = df_model['Date'].max()
    target_date = pd.Timestamp('2027-12-31')
    
    # Generate future dates
    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), end=target_date, freq='D')
    future_days = (future_dates - df_model['Date'].min()).days.values.reshape(-1, 1)
    
    # Predict future prices
    future_prices = lr_model.predict(future_days)
    
    # Combine historical and future data
    future_df = pd.DataFrame({
        'Date': future_dates,
        'Predicted_Price': future_prices
    })
    
    # Visualization: Historical + Predictions
    st.write("### üìä Bi·ªÉu ƒë·ªì d·ª± ƒëo√°n gi√° v√†ng (Gold Price Prediction Chart)")
    
    fig, ax = plt.subplots(figsize=(14, 6))
    
    # Historical actual prices
    ax.plot(df_model['Date'], df_model['Close/Last'], 
            linewidth=2, color='steelblue', label='Historical Actual Price', alpha=0.8)
    
    # Historical predicted (fitted line)
    ax.plot(df_model['Date'], y_pred_train, 
            linewidth=2, color='orange', linestyle='--', label='Linear Regression Fit', alpha=0.7)
    
    # Future predictions
    ax.plot(future_df['Date'], future_df['Predicted_Price'], 
            linewidth=2.5, color='red', linestyle='-', label='Future Prediction (to 2027)', alpha=0.8)
    
    # Add confidence interval (simple approach)
    std_error = np.std(y - y_pred_train)
    ax.fill_between(future_df['Date'], 
                    future_df['Predicted_Price'] - 1.96*std_error,
                    future_df['Predicted_Price'] + 1.96*std_error,
                    alpha=0.2, color='red', label='95% Confidence Interval')
    
    ax.set_xlabel("Date", fontsize=12, fontweight='bold')
    ax.set_ylabel("Price (USD/oz)", fontsize=12, fontweight='bold')
    ax.set_title("Gold Price Prediction using Linear Regression (Historical + Future)", fontsize=14, fontweight='bold')
    ax.legend(loc='best', fontsize=10)
    ax.grid(True, alpha=0.3)
    plt.xticks(rotation=45)
    plt.tight_layout()
    st.pyplot(fig)
    
    # Show prediction statistics
    st.write("### üìä Th·ªëng k√™ d·ª± ƒëo√°n (Prediction Statistics)")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**D·ª± ƒëo√°n gi√° v√†ng:**")
        prediction_stats = {
            "Date": ["Last Historical", "End of 2025", "End of 2026", "End of 2027"],
            "Predicted Price": [
                f"${df_model['Close/Last'].iloc[-1]:.2f}",
                f"${lr_model.predict([[((pd.Timestamp('2025-12-31') - df_model['Date'].min()).days)]])[0]:.2f}",
                f"${lr_model.predict([[((pd.Timestamp('2026-12-31') - df_model['Date'].min()).days)]])[0]:.2f}",
                f"${lr_model.predict([[((pd.Timestamp('2027-12-31') - df_model['Date'].min()).days)]])[0]:.2f}"
            ]
        }
        st.dataframe(pd.DataFrame(prediction_stats), use_container_width=True)
    
    with col2:
        st.write("**Th√¥ng s·ªë m√¥ h√¨nh:**")
        model_params = {
            "Parameter": ["Slope (H·ªá s·ªë g√≥c)", "Intercept (H·∫±ng s·ªë)", "Daily Price Change"],
            "Value": [
                f"{lr_model.coef_[0]:.4f}",
                f"${lr_model.intercept_:.2f}",
                f"${lr_model.coef_[0]:.4f}/day"
            ]
        }
        st.dataframe(pd.DataFrame(model_params), use_container_width=True)
    
    # Model equation
    st.write("### üìê Ph∆∞∆°ng tr√¨nh h·ªìi quy (Regression Equation)")
    st.latex(f"Price = {lr_model.intercept_:.2f} + {lr_model.coef_[0]:.4f} \\times Days")
    
    # Interpretation
    st.write("### üí° Gi·∫£i th√≠ch k·∫øt qu·∫£ (Interpretation)")
    st.write(f"""
    **√ù nghƒ©a c√°c ch·ªâ s·ªë:**
    - **R¬≤ = {r2:.4f}**: M√¥ h√¨nh gi·∫£i th√≠ch {r2*100:.2f}% s·ª± bi·∫øn ƒë·ªông c·ªßa gi√° v√†ng {'‚úì (T·ªët)' if r2 > 0.7 else '‚ö† (Trung b√¨nh)' if r2 > 0.5 else '‚úó (Y·∫øu)'}
    - **RMSE = ${rmse:.2f}**: Sai s·ªë trung b√¨nh kho·∫£ng ${rmse:.2f}
    - **Slope = {lr_model.coef_[0]:.4f}**: Gi√° v√†ng {'tƒÉng' if lr_model.coef_[0] > 0 else 'gi·∫£m'} trung b√¨nh ${abs(lr_model.coef_[0]):.4f}/ng√†y
    
    **Xu h∆∞·ªõng:**
    {f"üìà Gi√° v√†ng c√≥ xu h∆∞·ªõng tƒÉng ƒë·ªÅu ƒë·∫∑n v·ªõi t·ªëc ƒë·ªô ${lr_model.coef_[0]*365:.2f}/nƒÉm" if lr_model.coef_[0] > 0 else f"üìâ Gi√° v√†ng c√≥ xu h∆∞·ªõng gi·∫£m v·ªõi t·ªëc ƒë·ªô ${abs(lr_model.coef_[0]*365):.2f}/nƒÉm"}
    
    **L∆∞u √Ω:** ‚ö†Ô∏è D·ª± ƒëo√°n d√†i h·∫°n v·ªõi Linear Regression c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c do gi·∫£ ƒë·ªãnh xu h∆∞·ªõng tuy·∫øn t√≠nh. 
    Gi√° v√†ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi nhi·ªÅu y·∫øu t·ªë kinh t·∫ø, ch√≠nh tr·ªã ph·ª©c t·∫°p.
    """)
    
    # Download prediction data
    st.write("### üíæ T·∫£i d·ªØ li·ªáu d·ª± ƒëo√°n (Download Prediction Data)")
    
    # Combine all data for download
    download_df = pd.DataFrame({
        'Date': list(df_model['Date']) + list(future_df['Date']),
        'Actual_Price': list(df_model['Close/Last']) + [np.nan]*len(future_df),
        'Predicted_Price': list(y_pred_train) + list(future_df['Predicted_Price']),
        'Type': ['Historical']*len(df_model) + ['Future']*len(future_df)
    })
    
    csv_data = download_df.to_csv(index=False)
    st.download_button(
        label="üì• Download Prediction CSV",
        data=csv_data,
        file_name="gold_price_prediction_to_2027.csv",
        mime="text/csv"
    )
